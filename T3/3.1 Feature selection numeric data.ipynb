{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf92331",
   "metadata": {
    "id": "7cf92331"
   },
   "source": [
    "# Feature Selection – Numeric data  \n",
    "### Introduction  \n",
    "When we work with data in table form—like a spreadsheet—each row usually represents one person or thing, and each column is a piece of information about them. For example, a person might have columns like age, weight, blood pressure, or glucose level.\n",
    "\n",
    "But not every column is useful when we’re trying to train a machine learning model. Some columns might be irrelevant or even distracting. *Feature selection* is the process of choosing only the columns that really help with the prediction task.\n",
    "\n",
    "Doing this can:\n",
    "\n",
    "- Make the model faster and more accurate  \n",
    "- Avoid “overthinking” (overfitting) by the model  \n",
    "- Make the results easier to understand\n",
    "\n",
    "We’ll look at a few simple ways to do this:\n",
    "\n",
    "- *Filter-based*: Use basic maths to decide which columns are not useful (e.g. if they don’t change much or are not related to the outcome).  \n",
    "- *Wrapper-based*: Try out different combinations of columns and see which ones work best with the model.  \n",
    "- *Embedded*: Let the model itself choose what’s useful while it trains.  \n",
    "- *PCA (Principal Component Analysis)*: A special technique that combines and simplifies columns into fewer, more powerful ones.\n",
    "\n",
    "We’ll show how this works using a dataset about diabetes risk in women, but these techniques can be used with all kinds of number-based data—like in health, finance, or engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48862f8f",
   "metadata": {
    "id": "48862f8f"
   },
   "source": [
    "### Installing Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee685061",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9269,
     "status": "ok",
     "timestamp": 1741819261909,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "ee685061",
    "outputId": "00d9da71-fdad-43a5-d98e-571c28c620f2"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "\n",
    "!pip install pandas numpy matplotlib scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "973f113b",
   "metadata": {},
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a744ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/martyn-harris-bbk/AppliedMachineLearning/main/data/pima-indians-diabetes.data.csv'\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "print(\"Download complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04d4d3",
   "metadata": {
    "id": "7a04d4d3"
   },
   "source": [
    "### Load the data\n",
    "\n",
    "When we load the dataset, and separate it into two distinct parts. The first eight columns become our *features*, stored in the variable `X`. These are the measurements our machine learning model will use to learn patterns. The final column is the *target*, and we store the rows in variable `Y`. This target indicates whether the patient has diabetes (`1`) or not (`0`) based on the features.\n",
    "\n",
    "Once loaded, the features (`X`) will then be used as input data to train the machine learning model, while the target variable (`Y`) is the outcome the model will try to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7361dd1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1741819262005,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "e7361dd1",
    "outputId": "e5973d15-6464-45f0-bd73-221da4897e8a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "header = [\n",
    "    'Pregnancy_Count',\n",
    "    'Glucone_conc',\n",
    "    'Blood_pressure',\n",
    "    'Skin_thickness',\n",
    "    'Insulin',\n",
    "    'BMI',\n",
    "    'DPF',\n",
    "    'Age',\n",
    "    'Class'\n",
    "]\n",
    "\n",
    "data = pd.read_csv(url, names=header)\n",
    "\n",
    "data.head()\n",
    "\n",
    "array = data.values\n",
    "\n",
    "X = array[:, 0:8]\n",
    "Y = array[:, 8]\n",
    "\n",
    "print(\"Data shape:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5578f7d",
   "metadata": {
    "id": "b5578f7d"
   },
   "source": [
    "Above, each row is one patient with numeric features like pregnancy count, glucose level, blood pressure, etc. The final column `class` indicates whether the patient had diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e9d252",
   "metadata": {
    "id": "a1e9d252"
   },
   "source": [
    "## Filter-based methods  \n",
    "\n",
    "Filter-based methods are a quick and simple way to decide which columns of data are useful before we train any machine learning model. They use basic statistics to score or rank each column (or *feature*) on its own, without involving any model at this stage.\n",
    "\n",
    "These methods help us:\n",
    "\n",
    "- Remove features that do not vary much or are not helpful  \n",
    "- Make the model faster and easier to train  \n",
    "- Reduce the chances of overfitting (when a model learns noise instead of useful patterns)\n",
    "\n",
    "Because they do not rely on any specific type of model, they can be used on many different kinds of datasets.\n",
    "\n",
    "Two common examples are:\n",
    "\n",
    "- *Variance threshold*: Removes features that barely change across the data  \n",
    "- *Chi-square test*: Looks at the relationship between each feature and the outcome to see if it is useful"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b42217c",
   "metadata": {
    "id": "6b42217c"
   },
   "source": [
    "### Variance threshold\n",
    "\n",
    "The variance threshold method removes features with low variance, under the assumption that features with little variability carry minimal predictive power. This is particularly useful in high-dimensional datasets where constant or near-constant features add noise without contributing meaningful information. The chi-square test, on the other hand, is useful for categorical data, measuring the statistical dependence between a feature and the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce99c28",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33,
     "status": "ok",
     "timestamp": 1741819262038,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "8ce99c28",
    "outputId": "cb4556f5-794a-411c-9cd3-b3e32fbb260f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_selection import VarianceThreshold  \n",
    "\n",
    "# Define a threshold for minimum variance\n",
    "threshold_n = 0.75\n",
    "\n",
    "# Create a VarianceThreshold object using the formula: p * (1 - p)\n",
    "# This is useful for binary features, where p is the proportion of ones (or zeros)\n",
    "sel = VarianceThreshold(threshold=(threshold_n * (1 - threshold_n)))\n",
    "\n",
    "# Fit the selector to the dataset X (learn variances of each feature)\n",
    "vt = sel.fit(X)\n",
    "\n",
    "# Print the actual variance of each feature in X\n",
    "print(\"Feature variances:\", vt.variances_)\n",
    "\n",
    "# Create a mask showing which features have variance higher than the threshold\n",
    "mask = vt.variances_ > threshold_n\n",
    "\n",
    "# Print a boolean array indicating which features are kept (True) and which are removed (False)\n",
    "print(\"Kept features?\", mask)\n",
    "\n",
    "# Find the indices (positions) of the features that are kept\n",
    "idx = np.where(mask == True)[0]\n",
    "\n",
    "# Print the indices of the features that were retained\n",
    "print(\"Indices of kept features:\", idx)\n",
    "\n",
    "# Apply the filter: keep only high-variance features\n",
    "X_highvar = vt.transform(X)\n",
    "\n",
    "# Show how the shape of the data has changed after filtering\n",
    "print(\"Original shape:\", X.shape, \"-> After high-variance filter:\", X_highvar.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dcfc75",
   "metadata": {
    "id": "c9dcfc75"
   },
   "source": [
    "This code uses sklearn's `VarianceThreshold` to filter out low-variance features from a dataset `X`. The threshold is set to *0.75 × (1 - 0.75) = 0.1875*, meaning that only features with a variance greater than 0.1875 will be kept. Any feature with a variance less than or equal to 0.1875 will be removed from the dataset.\n",
    "\n",
    "After fitting the transformer (`sel.fit(X)`), it prints the computed variances of all features and determines which ones exceed the threshold.\n",
    "\n",
    "It then identifies the indices of the retained high-variance features and applies the transformation (`vt.transform(X)`) to create a reduced dataset, `X_highvar`. Finally, it prints the original and new dataset shapes, showing how many features were removed.\n",
    "\n",
    "The variances of all eight features were listed, revealing that feature 6 had the lowest variance (0.1096), making it the only one to be dropped. The remaining features had sufficiently high variance and were retained for further processing.\n",
    "\n",
    "As a result, the dataset's shape changed from (768, 8) to (768, 7), meaning one feature was eliminated while the other seven were preserved. This filtering step helps reduce noise and improve model efficiency by discarding low-variance features that contribute little useful information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5680e87f",
   "metadata": {
    "id": "5680e87f"
   },
   "source": [
    "### Univariate selection (Chi-Square)\n",
    "Univariate selection is a statistical feature selection technique that evaluates each feature individually to determine its relevance to the target variable. It uses statistical tests to assign scores to features, helping to identify those that have the strongest relationship with the dependent variable.\n",
    "\n",
    "This approach is particularly useful for initial feature filtering in high-dimensional datasets, allowing for a quick reduction in the number of features before applying more complex selection methods. However, as it evaluates features independently, it may overlook interactions between them, which could be important for predictive performance.\n",
    "\n",
    "We measure each feature’s relationship to the target via a **chi-square** test. Then we keep the top `k` features with the highest chi-square scores.  High chi-square values indicate strong relationships, helping to prioritise the most informative features for classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55ea73d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 67,
     "status": "ok",
     "timestamp": 1741819262078,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "a55ea73d",
    "outputId": "e4890668-3bda-4b4c-b130-163bbaa46c12"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2 \n",
    "\n",
    "# Create a SelectKBest object to select the top 4 features using the chi-squared test\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "\n",
    "# Fit the feature selector to the data (X: input features, Y: target labels)\n",
    "fit = test.fit(X, Y)\n",
    "\n",
    "# Print the chi-squared score for each feature (higher score = more relevant)\n",
    "print(\"Chi2 scores per feature:\", fit.scores_)\n",
    "\n",
    "# Transform the dataset to include only the top 4 selected features\n",
    "features = fit.transform(X)\n",
    "print(\"Shape after SelectKBest:\", features.shape)  # Show the new shape of the dataset\n",
    "\n",
    "# Get the indices (positions) of the selected features\n",
    "cols = fit.get_support(indices=True)\n",
    "print(\"Indices of selected features (chi2):\", cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6cae37",
   "metadata": {
    "id": "3b6cae37"
   },
   "source": [
    "Now we’ve dropped some columns based on univariate statistics alone. Here are selected features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bdcbfa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1741819262078,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "91bdcbfa",
    "outputId": "06657941-8521-4ee9-8863-4c12df4599ad"
   },
   "outputs": [],
   "source": [
    "feature_names = header[:8]\n",
    "selected_feature_names = [feature_names[i] for i in cols]\n",
    "selected_feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082d2ef7",
   "metadata": {
    "id": "082d2ef7"
   },
   "source": [
    "While filter methods are useful for preliminary feature selection, they may overlook interactions between features that more sophisticated methods, such as wrapper or embedded approaches, can capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd0e07a",
   "metadata": {
    "id": "bcd0e07a"
   },
   "source": [
    "## Wrapper-based methods\n",
    "\n",
    "Wrapper methods are a feature selection approach that iteratively train a machine learning model on different subsets of features to evaluate their impact on performance. Unlike filter methods, which rely on statistical measures, wrapper methods assess feature importance based on the predictive power of a model, making them more tailored to specific learning algorithms. This often leads to better results, as the selected features are optimised for the model in use.  \n",
    "\n",
    "The process typically involves searching for the best subset of features using strategies such as *forward selection*, *backward elimination*, or *recursive feature elimination (RFE)*. Forward selection starts with no features and adds the most informative ones step by step, while backward elimination begins with all features and removes the least important ones iteratively. RFE, commonly used with models like decision trees or support vector machines, repeatedly removes the least significant features while retraining the model until an optimal subset remains.  \n",
    "\n",
    "Although wrapper methods can significantly improve model performance, they tend to be computationally expensive, especially for large datasets with many features. This is because each iteration requires training and evaluating the model, making these methods impractical for very high-dimensional data. As a result, they are often used when computational resources allow, or after an initial filtering step has reduced the feature space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69a5c4a",
   "metadata": {
    "id": "b69a5c4a"
   },
   "source": [
    "### Recursive Feature Elimination (RFE)  \n",
    "Recursive Feature Elimination (RFE) is a wrapper-based feature selection method that iteratively removes the least important features to improve model performance. It works by training a machine learning model, ranking features based on their importance, and systematically eliminating the weakest ones until the desired number of features remains. This process ensures that only the most relevant and informative features are retained, helping to reduce overfitting and improve model efficiency.  \n",
    "\n",
    "In this case, RFE is applied using logistic regression as the base classifier. Logistic regression assigns importance to features based on their coefficients, allowing RFE to rank and prune them accordingly. The algorithm follows these steps:  \n",
    "\n",
    "1. *Initial Training* – The logistic regression model is trained on all features.  \n",
    "2. *Feature Ranking* – Each feature is assigned an importance score, typically based on the absolute value of its coefficient.  \n",
    "3. *Elimination* – The least significant feature(s) are removed from the dataset.  \n",
    "4. *Reiteration* – Steps 1–3 are repeated with the remaining features until the specified number of final features is reached.  \n",
    "\n",
    "Below, RFE is configured to select the top 3 most important features, meaning the process stops once only three remain. This technique is particularly useful for high-dimensional datasets where many features may be redundant or irrelevant. However, RFE can be computationally expensive, especially when used with complex models or large datasets, as it requires multiple iterations of training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e675d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 32,
     "status": "ok",
     "timestamp": 1741819262125,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "788e675d",
    "outputId": "330e0e63-8353-4f7e-9837-67ffb023d927"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE     \n",
    "from sklearn.linear_model import LogisticRegression      # Base model for evaluating feature importance\n",
    "\n",
    "# Create a logistic regression model (used to evaluate features)\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Create an RFE object to select the top 3 features\n",
    "rfe = RFE(estimator=model, n_features_to_select=3)\n",
    "\n",
    "# Fit the RFE selector to the data (X: input features, Y: target labels)\n",
    "fit_rfe = rfe.fit(X, Y)\n",
    "\n",
    "# Print a boolean array showing which features were selected (True = kept)\n",
    "print(\"Selected (True=kept) features:\", fit_rfe.support_)\n",
    "\n",
    "# Print the ranking of all features (1 = most important, higher = less important)\n",
    "print(\"Feature ranking (1=most important):\", fit_rfe.ranking_)\n",
    "\n",
    "# Get the indices of the features that were selected\n",
    "kept_indices = np.where(fit_rfe.support_ == True)[0]\n",
    "\n",
    "# Get the names of the selected features using the original column headers\n",
    "kept_features = [header[i] for i in kept_indices]\n",
    "\n",
    "# Display the names of the kept features\n",
    "kept_features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcfb749",
   "metadata": {
    "id": "abcfb749"
   },
   "source": [
    "This reveals which numeric attributes RFE kept, given a logistic regression backbone.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee78d922",
   "metadata": {
    "id": "ee78d922"
   },
   "source": [
    "## Embedded methods  \n",
    "\n",
    "Embedded methods are a feature selection approach that integrates the selection process within the model training phase. Unlike filter methods, which rely on statistical properties, or wrapper methods, which iteratively test feature subsets, embedded methods select features as the model learns. This allows them to balance computational efficiency with improved predictive power by considering the relationships between features and the target variable.  \n",
    "\n",
    "One common approach in embedded methods is *regularisation*, which penalises less important features by shrinking their coefficients toward zero, effectively removing them from the model. Examples include *Lasso regression (L1 regularisation)*, which forces some coefficients to become exactly zero, and *Ridge regression (L2 regularisation)*, which reduces the magnitude of coefficients without eliminating them entirely. These techniques help prevent overfitting while ensuring only the most relevant features contribute to the model.  \n",
    "\n",
    "Another approach leverages built-in feature importance scores from tree-based models such as decision trees, random forests, and gradient boosting methods (e.g., XGBoost, LightGBM). These models rank features based on how much they reduce impurity (e.g., Gini impurity or entropy) in classification tasks or how much they reduce variance in regression tasks. Features with the lowest importance scores can be discarded, simplifying the model while maintaining performance.  \n",
    "\n",
    "Embedded methods strike a balance between the computational efficiency of filter methods and the predictive power of wrapper methods. They are particularly useful when working with large datasets, as they allow for feature selection without requiring repeated model retraining, making them a practical choice for many machine learning applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fd939f",
   "metadata": {
    "id": "c0fd939f"
   },
   "source": [
    "### L1-based selection  \n",
    "\n",
    "L1-based feature selection, commonly used in *logistic regression*, applies *L1 regularisation (Lasso)* to shrink some feature coefficients to exactly zero, effectively removing them from the model. This is achieved by setting `penalty='l1'` when using a solver that supports it, such as `'liblinear'`.\n",
    "\n",
    "The `C` parameter controls the strength of regularisation, where smaller values increase regularisation, leading to more coefficients being set to zero.\n",
    "\n",
    "This method is useful for automatic feature selection, reducing model complexity, and improving interpretability while maintaining predictive performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214fe824",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1741819262245,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "214fe824",
    "outputId": "f236f4cb-4adc-4c10-e91a-ec651f17b6f2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define a list of regularisation strengths to try\n",
    "# Smaller C means stronger regularisation (more penalty on complexity)\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "# List to store how many features are selected for each C value\n",
    "num_features_selected = []\n",
    "\n",
    "# Check if X is a DataFrame (has named columns), otherwise create default feature names\n",
    "feature_names = X.columns if isinstance(X, pd.DataFrame) else [header[i] for i in range(X.shape[1])]\n",
    "\n",
    "# Loop through each value of C (controls the strength of regularisation)\n",
    "for c in C_values:\n",
    "    # Create a logistic regression model using L1 penalty (Lasso-like, drives some coefficients to zero)\n",
    "    lr_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=c)\n",
    "\n",
    "    # Fit the model to the data\n",
    "    lr_l1.fit(X, Y)\n",
    "\n",
    "    # Get the learned coefficients (one per feature)\n",
    "    coef = lr_l1.coef_[0]  # Shape is (n_features,)\n",
    "\n",
    "    # Identify features where the coefficient is non-zero (i.e., selected by the model)\n",
    "    non_zero_idx = np.where(coef != 0)[0]\n",
    "\n",
    "    # Get the corresponding feature names for non-zero coefficients\n",
    "    selected_features = [feature_names[i] for i in non_zero_idx]\n",
    "\n",
    "    # Record the number of features selected for this value of C\n",
    "    num_features_selected.append(len(selected_features))\n",
    "\n",
    "    # Print the selected features for this model\n",
    "    print(f\"C = {c}: Selected Features ({len(selected_features)}): {selected_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513137ca",
   "metadata": {},
   "source": [
    "Let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b0ee88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 432
    },
    "executionInfo": {
     "elapsed": 978,
     "status": "ok",
     "timestamp": 1741819263222,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "20b0ee88",
    "outputId": "641df219-1f98-4a35-9d7b-4bb6b6675c62"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "\n",
    "plt.plot(C_values, num_features_selected, marker='o')\n",
    "\n",
    "plt.xscale('log')\n",
    "\n",
    "plt.xlabel('C (log-scale)')\n",
    "plt.ylabel('Number of non-zero coefficients')\n",
    "\n",
    "plt.title('L1-based Feature Selection')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad88dd5a",
   "metadata": {},
   "source": [
    "The following code summarises the results by printing the number of non-zero (i.e. selected) features for each value of C, helping us observe the trade-off between model simplicity and the number of features used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c603028e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1741819263226,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "c603028e",
    "outputId": "3f57e811-3d9c-4bef-cfb8-5b92567ae8c9"
   },
   "outputs": [],
   "source": [
    "for c, nf in zip(C_values, num_features_selected):\n",
    "    print(f\"C={c}: {nf} non-zero features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31142bc8",
   "metadata": {
    "id": "31142bc8"
   },
   "source": [
    "A *smaller* `C` means *stronger* L1 penalty, hence more features driven to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c9cf38",
   "metadata": {
    "id": "e8c9cf38"
   },
   "source": [
    "### Tree-based feature importances\n",
    "Tree-based algorithms, such as Decision Trees, Random Forests, Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost), and Extra Trees, provide a built-in mechanism for evaluating feature importance. Unlike statistical or wrapper-based methods, these models inherently assess the contribution of each feature to the prediction task during training. The importance score for each feature is derived from how frequently it is used in splitting nodes and how much it reduces the impurity (e.g., Gini impurity or entropy in classification, or variance reduction in regression).\n",
    "\n",
    "In Random Forests, which aggregate multiple decision trees, the feature importance is computed by averaging the impurity reduction across all trees in the ensemble. Features that frequently result in significant error reduction are assigned higher importance scores. Similarly, Gradient Boosting models, such as XGBoost and LightGBM, calculate feature importance based on how much each feature contributes to reducing the model’s overall loss function across multiple boosting iterations.\n",
    "\n",
    "One advantage of tree-based feature importance is that it captures feature interactions—unlike simpler methods that assess features independently. However, these importance scores can sometimes be biased toward features with higher cardinality (i.e., features with more unique values). Additionally, while they provide insight into which features contribute most to the model, they do not inherently perform feature selection. If needed, features with very low importance can be removed to improve model efficiency without significantly impacting performance.\n",
    "\n",
    "In scikit-learn, you can obtain feature importances from a trained Random Forest model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb65a0b",
   "metadata": {
    "id": "1eb65a0b"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create a Random Forest classifier with 100 trees\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model on the full dataset (X: features, Y: labels)\n",
    "rf.fit(X, Y)\n",
    "\n",
    "# Get the importance of each feature as calculated by the trained model\n",
    "# Higher values mean the feature had more influence on predictions\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create a dictionary mapping feature names to their importance scores\n",
    "# Assumes the first 8 column names are stored in `header[:8]`\n",
    "feature_importance_dict = dict(zip(header[:8], importances))\n",
    "\n",
    "# Loop through each feature and its importance, printing the results\n",
    "for fname, imp in feature_importance_dict.items():\n",
    "    print(fname, \"->\", round(imp, 3))  # Round importance to 3 decimal places for readability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7b508",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "executionInfo": {
     "elapsed": 465,
     "status": "ok",
     "timestamp": 1741819264258,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "91e7b508",
    "outputId": "ce2294a2-c024-4fc3-fa08-8856e6f443f7"
   },
   "outputs": [],
   "source": [
    "plt.bar(range(len(importances)), importances, tick_label=header[:8])\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.title('Random Forest Feature Importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2a0242",
   "metadata": {
    "id": "5a2a0242"
   },
   "source": [
    "We can rank or threshold these importance scores to select key features if desired.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bcf551",
   "metadata": {
    "id": "85bcf551"
   },
   "source": [
    "### Dimensionality Reduction (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a popular dimensionality reduction technique that transforms a high-dimensional dataset into a smaller set of principal components. Each principal component is a linear combination of the original features and is designed to capture the maximum variance in the data. The goal of PCA is not to select a subset of existing features, but rather to create a new feature space that retains the most informative patterns while reducing redundancy. This makes it especially useful for datasets with many correlated variables, as PCA helps eliminate multicollinearity and improves model efficiency.\n",
    "\n",
    "Unlike traditional feature selection, which keeps a subset of original features, PCA projects data into a new coordinate system. This means that the transformed features (principal components) are not directly interpretable in terms of the original variables. However, they still preserve most of the information, making PCA particularly valuable for high-dimensional numeric datasets where reducing the number of features can enhance computational efficiency without significantly compromising performance.\n",
    "\n",
    "#### Steps to perform PCA on a data set\n",
    "- Standardise the data (if necessary) – Since PCA is affected by scale, it is common to normalise features so that each has zero mean and unit variance.\n",
    "- Compute the Covariance Matrix – This measures how features vary together.\n",
    "- Perform Eigenvalue Decomposition – The eigenvectors represent the principal components, while eigenvalues indicate the amount of variance each component captures.\n",
    "- Sort and select Principal Components – The components are ranked based on their explained variance, and only the top k components are kept, reducing the dimensionality of the dataset.\n",
    "- Lastly, choosing the number of components to use for our model.\n",
    "\n",
    "To determine the optimal number of principal components, we often look at the explained variance ratio, which indicates how much of the total variance each component captures. A common approach is to select enough components to explain at least 90–95% of the variance, balancing dimensionality reduction with information retention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4e766a",
   "metadata": {
    "id": "8c4e766a"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA  \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "# Standardise the feature data (mean = 0, standard deviation = 1)\n",
    "# This is important for PCA because it is sensitive to the scale of the features\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Create a PCA object (no components specified, so it will keep all of them)\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA to the scaled data and transform it\n",
    "# This projects the original features into a new set of uncorrelated components\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db56c1ab",
   "metadata": {
    "id": "db56c1ab"
   },
   "source": [
    "Let's visualise the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b83fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "executionInfo": {
     "elapsed": 914,
     "status": "ok",
     "timestamp": 1741819265176,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "509b83fd",
    "outputId": "b9f71875-30ee-4489-e7e1-9f9230eda180"
   },
   "outputs": [],
   "source": [
    "# Plot explained variance ratio\n",
    "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1),\n",
    "         pca.explained_variance_ratio_.cumsum(), marker='o', linestyle='--')\n",
    "\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "\n",
    "plt.title('Explained Variance by Number of Components')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdb1f42",
   "metadata": {
    "id": "abdb1f42"
   },
   "source": [
    "*Note*: Combining PCA with *scaling* is common if features have very different scales. So ensure you explore the dataset first, and determine whether this is required.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-compare",
   "metadata": {
    "id": "final-compare"
   },
   "source": [
    "### Comparing performance before and after\n",
    "Finally, let’s see if feature selection changes a model’s accuracy. We will train logistic regression on the following:\n",
    "\n",
    "1. All features 8 features.\n",
    "2. Top 6 features using chi-square.\n",
    "\n",
    "We will take a sample of our data for training, and one for testing (more on this later).\n",
    "We then create the model and train it on the training data and evaluate it on the test data to see how well it performed in predicting diabetes given the patient records (with all training data, and a reduced collection of selected columns, or features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b2c3cb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 62,
     "status": "ok",
     "timestamp": 1741819265239,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "21b2c3cb",
    "outputId": "c59d314c-4977-4e2f-b2b0-dda1d73922ad"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score  # For splitting data and cross-validation - more on this later\n",
    "from sklearn.linear_model import LogisticRegression  \n",
    "from sklearn.feature_selection import SelectKBest, chi2 \n",
    "\n",
    "# Split the dataset into training and test sets (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model using the 'liblinear' solver (suitable for small datasets)\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Train and evaluate using all features\n",
    "model.fit(X_train, y_train)  # Fit model on training data\n",
    "score_all = model.score(X_test, y_test)  # Evaluate accuracy on test set\n",
    "print(\"Accuracy (all features):\", score_all)\n",
    "\n",
    "# Train and evaluate using only top 6 features selected by Chi-squared test\n",
    "selector = SelectKBest(score_func=chi2, k=6)  # Select top 6 features based on chi2 scores\n",
    "\n",
    "# Fit selector on training data and transform both train and test sets\n",
    "Xk_train = selector.fit_transform(X_train, y_train)\n",
    "Xk_test = selector.transform(X_test)\n",
    "\n",
    "# Create a new logistic regression model for the reduced feature set\n",
    "model_k = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Train the model with the selected features\n",
    "model_k.fit(Xk_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test set with selected features\n",
    "score_k = model_k.score(Xk_test, y_test)\n",
    "print(\"Accuracy (top 6 chi2):\", score_k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92197776",
   "metadata": {
    "id": "92197776"
   },
   "source": [
    "Whether feature selection enhances model accuracy depends on multiple factors, including the dataset characteristics, the selection method used, and model tuning. In some cases, removing certain features improves generalisation by reducing overfitting, while in others, it may discard valuable information, leading to decreased performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d585f90",
   "metadata": {
    "id": "6d585f90"
   },
   "source": [
    "## What have we learnt?\n",
    "We covered filter-based, wrapper-based, and embedded feature selection techniques, along with PCA for dimensionality reduction. Filter-based methods quickly eliminate features using statistical measures like variance, correlation, or chi-square tests. Wrapper-based methods, such as Recursive Feature Elimination (RFE), iteratively evaluate feature subsets but can be computationally expensive. Embedded methods integrate feature selection within model training, as seen in L1-regularised logistic regression or tree-based models like random forests. PCA reduces dimensionality by projecting data into a smaller number of principal components, retaining variance but losing direct interpretability of original features.\n",
    "\n",
    "In practice, feature selection should be approached experimentally, testing multiple techniques to find the best balance between model accuracy, efficiency, and interpretability. These principles apply across various fields, including finance, bioinformatics, and predictive maintenance, where reducing irrelevant features can enhance performance and simplify analysis."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
