{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f489567",
   "metadata": {
    "id": "8f489567"
   },
   "source": [
    "## Feature Selection for Image data\n",
    "### Introduction\n",
    "Images are a rich source of information, with each pixel (and the relationships between pixels) contributing to the overall meaning. However, this can also make image data extremely high-dimensional. In practical terms, this means we have a vast number of features (pixels) to process, which can be both computationally expensive and prone to overfitting if not handled carefully.\n",
    "\n",
    "In this notebook, we will explore various techniques for identifying and retaining the most important features (or representations) in image data. By focusing on these features, we can often build more efficient and accurate models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e879ee",
   "metadata": {
    "id": "67e879ee"
   },
   "source": [
    "### Why conventional feature selection methods may not suffice\n",
    "\n",
    "Conventional methods such as variance thresholding, chi-squared tests, or straightforward recursive feature elimination are sometimes effective for numeric or text data. These methods often assume that features are largely independent from one another. In images, however, neighbouring pixels are usually correlated, meaning that the importance of one pixel can depend strongly on nearby pixels. Removing individual pixels based solely on variance or mutual correlation can fail to capture the distinctive patterns that frequently matter in images.\n",
    "\n",
    "In practice, the techniques we use for image data try to find higher-level patterns or important regions. For example, a few pixels that form the eye of a cat or dog might matter more than background areas of the image, and these pixels are best considered together.\n",
    "\n",
    "Below, we examine different ways to reduce the dimensionality or extract key features from images, ranging from classical methods (like entropy or mutual information) to more modern deep learning approaches (such as pre-trained convolutional neural networks or saliency-based methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffd8ce",
   "metadata": {
    "id": "c6ffd8ce"
   },
   "source": [
    "### Installing Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a190685",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3212,
     "status": "ok",
     "timestamp": 1741819971063,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "8a190685",
    "outputId": "33b038f5-9acd-499f-cc63-b1fb8212529d"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "\n",
    "!pip install pandas numpy matplotlib opencv-python Pillow tensorflow scikit-image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c13274",
   "metadata": {
    "id": "e6c13274"
   },
   "source": [
    "### Download the image data\n",
    "Let us assume we have two categories of images, *cats* and *dogs*, and that our dataset is stored in a directory `./cats_dogs/train/`, which contains two subdirectories `cats/` and `dogs/` where each folder holds multiple images of the respective animal:\n",
    "\n",
    "```\n",
    "./cats_dogs/train/\n",
    "    cats/\n",
    "      cat_1.jpg\n",
    "      cat_2.jpg\n",
    "      ...\n",
    "    dogs/\n",
    "      dog_1.jpg\n",
    "      dog_2.jpg\n",
    "      ...\n",
    "```\n",
    "\n",
    "We will resize each image to 64x64 pixels, then flatten them into one-dimensional vectors for initial processing. Each element in the flattened array corresponds to a single pixel intensity (for colour images, we have three channels: red, green, and blue, each contributing one dimension per pixel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aYKFYFvhcMo",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "3aYKFYFvhcMo",
    "outputId": "fba5abfa-7b55-43ec-fbdb-333f7ac6ff58"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Define URL and target filenames\n",
    "url = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
    "zip_path = \"cats_and_dogs_filtered.zip\"\n",
    "extract_dir = \"cats_dogs\"\n",
    "\n",
    "# Download the zip file\n",
    "print(\"Downloading dataset...\")\n",
    "urllib.request.urlretrieve(url, zip_path)\n",
    "print(\"Download complete.\")\n",
    "\n",
    "# Extract the zip file\n",
    "print(\"Extracting files...\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "print(\"Extraction complete.\")\n",
    "\n",
    "# Delete the zip file\n",
    "print(\"Cleaning up...\")\n",
    "os.remove(zip_path)\n",
    "print(\"Cleanup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2083d39d",
   "metadata": {},
   "source": [
    "### Load the image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d3bcf7",
   "metadata": {
    "id": "87d3bcf7"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Set a fixed image size for resizing\n",
    "IMG_SIZE = 64\n",
    "\n",
    "dataset_path = \"cats_dogs/cats_and_dogs_filtered/train/\" # Path to the folder containing data\n",
    "\n",
    "category_path = [\"cats\", \"dogs\"]  # The subfolders - class labels\n",
    "\n",
    "# Lists to hold the image data and labels\n",
    "X = []  # Features\n",
    "Y = []  # Labels (0 for cats, 1 for dogs)\n",
    "\n",
    "limit = 100 # Limit the number of images to load per category to speed things up\n",
    "\n",
    "for label, category in enumerate(category_path):\n",
    "    path = os.path.join(dataset_path, category)\n",
    "    for img_name in os.listdir(path):\n",
    "        img = cv2.imread(os.path.join(path, img_name))\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "        if len(X) >= limit:\n",
    "            break\n",
    "\n",
    "        X.append(img)\n",
    "        Y.append(label)\n",
    "\n",
    "X = np.array(X, dtype=np.float32) / 255.0 # normalise\n",
    "Y = np.array(Y)\n",
    "\n",
    "print(f'Loaded {len(X)} images')\n",
    "print(f'Each image is resized to {IMG_SIZE}x{IMG_SIZE} pixels')\n",
    "print(f'Shape of X: {X.shape}')\n",
    "print(f'Shape of Y: {Y.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9b5966",
   "metadata": {
    "id": "ae9b5966"
   },
   "source": [
    "### Feature selection techniques for images\n",
    "When working with image data, one of the biggest challenges is its *high dimensionality* — each image contains thousands or even millions of pixels. Not all of this information is useful for making predictions, so we use *feature selection* techniques to focus only on the most important parts of the image.\n",
    "\n",
    "We will introduce a variety of methods for identifying or extracting the most relevant features from images:\n",
    "\n",
    "- *Feature Extraction using Pre-Trained CNNs*: These models, trained on large image datasets, can automatically detect patterns like edges, shapes, or textures that are helpful for classification.\n",
    "\n",
    "- *Saliency Maps and Grad-CAM*: These techniques highlight which parts of an image had the biggest influence on the model’s decision, helping us understand what the model is “looking at.”\n",
    "\n",
    "- *Entropy-Based Feature Selection*: Entropy measures the complexity or texture in different parts of an image, allowing us to focus on regions with more informative content.\n",
    "\n",
    "- *Mutual Information*: This is a statistical method that looks at how much each pixel or region contributes to predicting the target label.\n",
    "\n",
    "- *Dimensionality Reduction with Autoencoders*: Autoencoders are a type of neural network that learn to compress image data into a smaller, more useful representation, capturing only the most important information.\n",
    "\n",
    "Each of these approaches reduces the amount of data we need to work with, making models faster and more efficient — while still preserving the key information needed for accurate predictions. Some rely on deep learning to automatically learn important patterns, while others use statistical techniques to rank and select features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e354df67",
   "metadata": {
    "id": "e354df67"
   },
   "source": [
    "### Feature extraction using pre-trained CNNs\n",
    "\n",
    "A powerful and popular way to handle image data is to harness the power of a pre-trained convolutional neural network (CNN). Well-known networks like VGG16, ResNet, or MobileNet have been trained on massive datasets (such as ImageNet), allowing them to learn general-purpose representations of images.\n",
    "\n",
    "Instead of training a model from scratch on our smaller cat-and-dog dataset, we can use the internal layers of these pre-trained networks to extract discriminative features. This technique often provides a substantial boost in performance, even if the images are not from the exact same domain as the original training data.\n",
    "\n",
    "Below is an example using VGG16, where we:\n",
    "\n",
    "- load VGG16 (pre-trained on ImageNet) up to its last convolutional layer (i.e., excluding the final classifier layer).\n",
    "- pass our images through this truncated network, obtaining feature vectors describing each image.\n",
    "\n",
    "These feature vectors capture higher-level visual patterns (such as edges, shapes, or textures), which can then be used as input to a simpler classifier.\n",
    "\n",
    "*Note:* Using a pre-trained CNN can vastly reduce the number of features, because these models only output a moderate number of activation maps in their final convolutional layers, rather than tens of thousands of raw pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cbfb13",
   "metadata": {
    "id": "e1cbfb13"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Load the VGG16 model pre-trained on ImageNet\n",
    "# 'include_top=False' removes the fully connected layers at the top, so we only keep the convolutional part (feature extractor)\n",
    "# 'input_shape' defines the size of input images (must match IMG_SIZE and use 3 channels for RGB)\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Create a new model that outputs the convolutional feature maps from the original VGG16\n",
    "# We keep the same input, but output from the last convolutional layer\n",
    "model = Model(inputs=base_model.input, outputs=base_model.output)\n",
    "\n",
    "# Reshape the image data to match the model's expected input format:\n",
    "# -1 lets NumPy automatically calculate the number of samples\n",
    "# IMG_SIZE x IMG_SIZE defines height and width\n",
    "# 3 indicates the number of colour channels (RGB)\n",
    "X_reshaped = X.reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Print the shape of the reshaped input to verify dimensions\n",
    "print(\"X_reshaped shape:\", X_reshaped.shape)\n",
    "\n",
    "# Check for any missing (NaN) values in the image data, which could cause errors during prediction\n",
    "print(\"X_reshaped NaN values:\", np.isnan(X_reshaped).sum())\n",
    "\n",
    "# Print the minimum and maximum pixel values to check the value range\n",
    "# Pre-trained models like VGG typically expect pixel values in a certain range (e.g. 0–255 or 0–1)\n",
    "print(\"X_reshaped min/max:\", X_reshaped.min(), X_reshaped.max())\n",
    "\n",
    "# Pass the reshaped image data through the VGG16 model to extract feature maps\n",
    "# This produces a compressed, abstract representation of each image, capturing key visual features\n",
    "X_cnn = model.predict(X_reshaped)\n",
    "\n",
    "# Print the shape of the extracted features to understand how the images have been transformed\n",
    "print('Extracted feature shape:', X_cnn.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df7e396",
   "metadata": {
    "id": "1df7e396"
   },
   "source": [
    "The output provides key insights about the input data (X_reshaped) and the feature maps extracted by the VGG16 model.\n",
    "```\n",
    "X_reshaped shape: (21, 64, 64, 3)\n",
    "```\n",
    "We have 21 images, each of size 64×64 pixels with 3 color channels (RGB). And there are no missing or invalid values in the dataset.\n",
    "\n",
    "The pixel intensity ranges from 0 to 255, which suggests that the images may not be normalised (VGG16 usually expects values in the `[0, 1]` range or preprocessed using keras.applications.vgg16.preprocess_input).\n",
    "```\n",
    "Extracted feature shape: (21, 2, 2, 512)\n",
    "```\n",
    "The output for each image is a 2×2 feature map with 512 channels (depth) per pixel. Since we used  `include_top=False`, the fully connected layers were removed, and VGG16 outputs a downsampled feature representation of the input.\n",
    "\n",
    "### Feature map downsampling\n",
    "When we pass a 64×64 image through the VGG16 model, the output is much smaller — just a 2×2 grid. This happens because the model uses layers that shrink the image size step by step. These layers are designed to zoom out gradually, so the model focuses less on exact pixel positions and more on the overall patterns.\n",
    "\n",
    "Even though the final output is small, it is very rich in information. Instead of RGB colour channels, the output now has *512 feature channels*. Each of these captures a different type of pattern or texture the model has learned — like edges, shapes, or textures that are useful for recognising objects.\n",
    "\n",
    "Let’s now take a look at what these features look like by plotting them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c7046a",
   "metadata": {
    "id": "49c7046a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select the first image in the batch\n",
    "image_idx = 0\n",
    "feature_maps = X_cnn[image_idx]  # Shape: (2, 2, 512)\n",
    "\n",
    "# Select a few feature maps to visualise\n",
    "num_features = min(8, feature_maps.shape[-1])  # Show at most 8 feature maps\n",
    "fig, axes = plt.subplots(1, num_features, figsize=(10, 4))\n",
    "\n",
    "for i in range(num_features):\n",
    "    feature_map = feature_maps[:, :, i]  # Extract i-th feature map\n",
    "    axes[i].imshow(feature_map, cmap='inferno')\n",
    "    axes[i].axis(\"off\")\n",
    "    axes[i].set_title(f\"Feature {i+1}\")\n",
    "\n",
    "plt.suptitle(\"Sample Feature Maps from VGG16\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab9074",
   "metadata": {
    "id": "bfab9074"
   },
   "source": [
    "Each feature map captures different patterns (edges, textures, or complex features). The darker/lighter regions indicate which parts of the image activated that filter. To see which regions of the image activate the network the most, we can average the 512 feature maps and visualise the activation intensity.\n",
    "\n",
    "Bright areas equal strongly activated regions. Whereas, dark areas are weakly activated regions. This helps understand which parts of an image VGG16 considers important.\n",
    "\n",
    "These extracted features can be flattened further for classical machine learning algorithms, or used directly in a classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca4c7f6",
   "metadata": {
    "id": "aca4c7f6"
   },
   "outputs": [],
   "source": [
    "X_flattened = X_cnn.reshape(X_cnn.shape[0], -1)\n",
    "print(X_flattened.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33287fb",
   "metadata": {
    "id": "d33287fb"
   },
   "source": [
    "### Saliency maps and Grad-CAM\n",
    "\n",
    "Saliency maps and Grad-CAM are tools that help us *see* what a neural network is focusing on when it looks at an image.\n",
    "\n",
    "When a model makes a prediction — for example, saying an image is a cat — these methods highlight the parts of the image that were most important in making that decision.\n",
    "\n",
    "- *Saliency maps* work by measuring how much each pixel affects the model’s prediction. If changing a certain pixel would change the result, that pixel gets highlighted. This shows us which areas the model finds most \"sensitive\" or meaningful.\n",
    "\n",
    "- *Grad-CAM* builds on this idea by highlighting entire regions, rather than individual pixels. It shows us which parts of the image the model is “looking at” the most, giving a more human-friendly explanation of what influenced the prediction.\n",
    "\n",
    "In short, these techniques give us a kind of “heatmap” over the image, helping us understand why the model made its choice — a powerful way to build trust and transparency in AI systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e1407b",
   "metadata": {
    "id": "28e1407b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Load pre-trained VGG16 model\n",
    "model = tf.keras.applications.VGG16(weights=\"imagenet\")\n",
    "\n",
    "model.trainable = False  # Keep it fixed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6b70f",
   "metadata": {
    "id": "65e6b70f"
   },
   "source": [
    "#### Load and preprocess the image\n",
    "We write a simple helper function to load and preproces each image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9398c8",
   "metadata": {
    "id": "be9398c8"
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = np.expand_dims(img, axis=0)  # Add batch dimension\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img)  # Normalize like ImageNet\n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a087b302",
   "metadata": {
    "id": "a087b302"
   },
   "source": [
    "### Saliency map\n",
    "This next function generates a **saliency map**, which visually highlights the parts of an image that most influenced a model's prediction. The process begins by preprocessing the input image—resizing it, normalising pixel values, and formatting it so the model can use it. The image is then passed into a neural network, and the model makes a prediction. \n",
    "\n",
    "To understand which parts of the image contributed most to this prediction, the code uses something called a *gradient*, which measures how much a small change in each pixel would affect the model’s confidence in its top predicted class. The stronger the gradient, the more important that pixel is. The function then processes these gradients, reducing them into a 2D map that highlights the most \"influential\" areas. \n",
    "\n",
    "Finally, it scales the values between 0 and 1, so the map can be visualised as a clear and interpretable heatmap:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad7a23e",
   "metadata": {
    "id": "6ad7a23e"
   },
   "outputs": [],
   "source": [
    "def compute_saliency_map(image_path):\n",
    "    # Preprocess the image (e.g. resize, normalise, expand dims), custom function assumed\n",
    "    img = preprocess_image(image_path)\n",
    "\n",
    "    # Convert the preprocessed image to a TensorFlow tensor of type float32\n",
    "    img_tensor = tf.convert_to_tensor(img, dtype=tf.float32)\n",
    "\n",
    "    # Start recording operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Tell the tape to watch the image tensor, so we can compute gradients with respect to it\n",
    "        tape.watch(img_tensor)\n",
    "\n",
    "        # Make a prediction using the model\n",
    "        predictions = model(img_tensor)\n",
    "\n",
    "        # Identify the class with the highest predicted probability\n",
    "        top_class = tf.argmax(predictions[0])\n",
    "\n",
    "        # Define the \"loss\" as the model’s confidence in the top class\n",
    "        loss = predictions[:, top_class]\n",
    "\n",
    "    # Compute the gradient of the top class score with respect to the input image\n",
    "    grads = tape.gradient(loss, img_tensor)\n",
    "\n",
    "    # Take the absolute value of the gradients and reduce across the colour channels (RGB) to get a single 2D map\n",
    "    saliency = tf.reduce_max(tf.abs(grads), axis=-1)[0]\n",
    "\n",
    "    # Normalise the saliency map values between 0 and 1 for visualisation\n",
    "    saliency = (saliency - tf.reduce_min(saliency)) / (tf.reduce_max(saliency) - tf.reduce_min(saliency))\n",
    "\n",
    "    # Convert the result to a NumPy array and return\n",
    "    return saliency.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6636ef65",
   "metadata": {
    "id": "6636ef65"
   },
   "source": [
    "Lets now use this function and apply it to a sample image `dog.1.jpg`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650cf0b8",
   "metadata": {
    "id": "650cf0b8"
   },
   "outputs": [],
   "source": [
    "image_path = dataset_path + \"dogs/dog.1.jpg\"  # Provide an image path\n",
    "print(image_path)\n",
    "\n",
    "saliency = compute_saliency_map(image_path)\n",
    "original_img = cv2.imread(image_path)\n",
    "original_img = cv2.resize(original_img, (224, 224))\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Original image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB))\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Saliency map\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(saliency, cmap=\"hot\")\n",
    "plt.title(\"Saliency Map\")\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14dc8a8",
   "metadata": {
    "id": "a14dc8a8"
   },
   "source": [
    "This method computes gradients of the most confident class with respect to input pixels. It also, highlights important regions that strongly influence the model’s decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe68485f",
   "metadata": {
    "id": "fe68485f"
   },
   "source": [
    "### Grad-CAM heatmap\n",
    "Grad-CAM (Gradient-weighted Class Activation Mapping) produces a heatmap over the image, highlighting regions that strongly influence the model's output. \n",
    "\n",
    "It is particularly helpful for CNNs, where convolutional layers learn spatially meaningful patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc071dec",
   "metadata": {
    "id": "cc071dec"
   },
   "source": [
    "#### Load VGG16 model (without top layers)\n",
    "\n",
    "We use the VGG16 model, a well-known deep learning architecture trained on millions of images from the ImageNet dataset. Loading the model without the top (fully connected) layers, keeps just the convolutional part of the model. \n",
    "\n",
    "This part acts as a powerful *feature extractor*, capturing useful visual patterns like edges, textures, and shapes from the input images. Instead of making predictions, the model outputs these extracted features, which we can then use for tasks like image classification, clustering, or further analysis. \n",
    "\n",
    "This approach saves time and resources, as we benefit from knowledge the model has already learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1086563b",
   "metadata": {
    "id": "1086563b"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "model = tf.keras.applications.VGG16(weights=\"imagenet\")\n",
    "model.trainable = False\n",
    "\n",
    "# Select last convolutional layer for Grad-CAM\n",
    "last_conv_layer_name = \"block5_conv3\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f553f771",
   "metadata": {
    "id": "f553f771"
   },
   "source": [
    "#### Compute Grad-CAM heatmap\n",
    "The *Grad-CAM (Gradient-weighted Class Activation Mapping)* approach is a popular technique for visually explaining what a deep learning model is focusing on when it makes a prediction.\n",
    "\n",
    "The method works by identifying which parts of an image contributed most to the model’s decision. It does this by examining the *gradients* flowing back from the model’s output to a specific *convolutional layer*. These gradients are used to calculate the importance of each feature map (or channel) in that layer.\n",
    "\n",
    "Once the important channels are identified, the function combines them to create a *heatmap* that highlights the regions in the image that had the most influence on the predicted class. This heatmap is then normalised for visual clarity, allowing us to overlay it on the original image and see where the model was “looking” when it made its decision.\n",
    "\n",
    "In short, this approach helps us *interpret* deep learning models by revealing which visual features drove a particular classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ae1c5",
   "metadata": {
    "id": "2f6ae1c5"
   },
   "outputs": [],
   "source": [
    "def compute_gradcam(image_path, model, layer_name):\n",
    "    # Preprocess the input image (e.g., resize, normalise, expand dims)\n",
    "    img = preprocess_image(image_path)\n",
    "\n",
    "    # Create a modified model that outputs both:\n",
    "    # 1) The activation from the specified convolutional layer\n",
    "    # 2) The final prediction from the original model\n",
    "    grad_model = tf.keras.models.Model(\n",
    "        inputs=model.input,\n",
    "        outputs=[model.get_layer(layer_name).output, model.output]\n",
    "    )\n",
    "\n",
    "    # Record operations for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass through the model: get both the conv layer output and final prediction\n",
    "        conv_output, predictions = grad_model(img)\n",
    "\n",
    "        # Identify the class with the highest predicted probability\n",
    "        top_class = tf.argmax(predictions[0])\n",
    "\n",
    "        # Define the loss as the model’s confidence in the top class\n",
    "        loss = predictions[:, top_class]\n",
    "\n",
    "    # Compute the gradient of the class score with respect to the conv layer output\n",
    "    grads = tape.gradient(loss, conv_output)\n",
    "\n",
    "    # Global average pooling: average the gradients over height and width dimensions\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "\n",
    "    # Remove the batch dimension from the conv layer output\n",
    "    conv_output = conv_output[0]\n",
    "\n",
    "    # Multiply each channel in the conv layer by its corresponding pooled gradient (i.e., its importance)\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_output), axis=-1)\n",
    "\n",
    "    # Apply ReLU to keep only positive values (areas that positively influence the class score)\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "\n",
    "    # Normalise the heatmap values to range from 0 to 1\n",
    "    heatmap /= np.max(heatmap)\n",
    "\n",
    "    # Return the heatmap as a NumPy array\n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4242993",
   "metadata": {
    "id": "a4242993"
   },
   "source": [
    "Now we can generate a Grad-CAM heatmap, let's overlay it onto an original image, to make things flexible, we will create another function to do this `overlay_heatmap`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27991150",
   "metadata": {
    "id": "27991150"
   },
   "outputs": [],
   "source": [
    "def overlay_heatmap(image_path, heatmap, alpha=0.4):\n",
    "    img = cv2.imread(image_path)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "\n",
    "    # Resize heatmap to match image size\n",
    "    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n",
    "\n",
    "    # Convert heatmap to color\n",
    "    heatmap = np.uint8(255 * heatmap)  # Scale to 0-255\n",
    "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)  # Apply colour map\n",
    "\n",
    "    # Overlay heatmap on original image\n",
    "    superimposed_img = cv2.addWeighted(heatmap, alpha, img, 1 - alpha, 0)\n",
    "\n",
    "    # Display original and heatmap images\n",
    "    plt.figure(figsize=(8, 4))\n",
    "\n",
    "    # Original image\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # Grad-CAM overlay\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(\"Grad-CAM Heatmap\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0423a",
   "metadata": {
    "id": "82d0423a"
   },
   "source": [
    "Now we can execute both `compute_gradcam`, and `overlay_heatmp` on an image of our choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e90b7a4",
   "metadata": {
    "id": "5e90b7a4"
   },
   "outputs": [],
   "source": [
    "image_path = dataset_path + \"dogs/dog.1.jpg\"  # Provide an image path\n",
    "\n",
    "heatmap = compute_gradcam(image_path, model, last_conv_layer_name)\n",
    "\n",
    "overlay_heatmap(image_path, heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4d865f",
   "metadata": {
    "id": "6f4d865f"
   },
   "source": [
    "While these methods are not strictly feature selection techniques in the traditional sense, they are useful for understanding and manually selecting important image regions, thereby potentially reducing unnecessary features or focusing on the most relevant parts of an image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd533290",
   "metadata": {
    "id": "fd533290"
   },
   "source": [
    "### Entropy-based feature selection\n",
    "\n",
    "Entropy is a measure of uncertainty or unpredictability within data. In our case, it is a measure of visual complexity or texture within an image and can be used to identify informative regions.\n",
    "\n",
    "This approach allows us to represent each image numerically based on its *textural content*, which can be useful for image classification or pattern analysis tasks.\n",
    "\n",
    "A pixel with low variance or repetitive values across many images may provide very little information to distinguish between classes. By contrast, pixels or regions with high entropy might capture essential variations.\n",
    "\n",
    "1. We convert the image to greyscale.\n",
    "2. We compute the local entropy of each pixel, often using a small neighbourhood (for instance, using a disk-shaped structuring element as provided by `skimage.morphology.disk`).\n",
    "3. We can then selectively retain pixels or regions that have higher entropy values.\n",
    "\n",
    "Below, we illustrate how to compute the entropy for each pixel using `skimage`. In a real-world scenario, you might:\n",
    "- Convert these per-pixel entropy values into a single statistic per region.\n",
    "- Select the top x% of pixels with the highest entropy.\n",
    "- Or threshold the entropy to keep only sufficiently informative pixels.\n",
    "\n",
    "Because this approach still involves per-pixel computations, it might be slow or unwieldy for large datasets. Nonetheless, it is a straightforward and interpretable starting point.\n",
    "\n",
    "Again, we create our own function `compute_entropy` to easily apply it to any image in our collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c7edab",
   "metadata": {
    "id": "19c7edab"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "from skimage.filters.rank import entropy\n",
    "from skimage.morphology import disk\n",
    "from skimage.transform import resize\n",
    "\n",
    "IMG_SIZE = 128  # Define a fixed image size to standardise input\n",
    "\n",
    "def compute_entropy(img_path):\n",
    "    # Read the image from the given file path\n",
    "    img = cv2.imread(img_path)\n",
    "\n",
    "    # Check if the image was successfully loaded\n",
    "    if img is None:\n",
    "        print(f\"Error: Unable to read image {img_path}\")\n",
    "        return None\n",
    "\n",
    "    # Convert the image to grayscale since entropy is computed on intensity values\n",
    "    grayscale = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Resize the image to a consistent size (128x128)\n",
    "    # This ensures all output feature vectors are the same length\n",
    "    grayscale = cv2.resize(grayscale, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "    # Ensure the image data type is 8-bit unsigned integers (required by skimage's entropy function)\n",
    "    grayscale = grayscale.astype(np.uint8)\n",
    "\n",
    "    # Compute local entropy using a circular neighbourhood with radius 5 pixels\n",
    "    ent = entropy(grayscale, disk(5))\n",
    "\n",
    "    # Flatten the 2D entropy matrix into a 1D array to use as a feature vector\n",
    "    return ent.flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a006a3b0",
   "metadata": {},
   "source": [
    "Now we can extract *entropy features* from all cat images in the dataset. \n",
    "\n",
    "To do this, our code will perform the following steps:\n",
    "\n",
    "- *Collect file paths*: We use `glob` to gather all `.jpg` files from the `cats` folder.  \n",
    "- *Loop through each image*: For each image, we apply the `compute_entropy()` function to calculate the local entropy values across the image.  \n",
    "- *Store features*: The resulting values (flattened into a 1D feature vector) are added to a list.  \n",
    "- *Convert to NumPy array*: This list is then converted into an array for use in modelling or analysis.  \n",
    "- *Inspect the result*: We print the shape of the final array to confirm the number of images and the length of each feature vector.\n",
    "\n",
    "Let's run this next cell, and see the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aac73d0",
   "metadata": {
    "id": "8aac73d0"
   },
   "outputs": [],
   "source": [
    "# Get all cat image paths\n",
    "cat_files = glob.glob(dataset_path + '/cats/*.jpg')\n",
    "\n",
    "X_entropy_cats = []\n",
    "\n",
    "for f in cat_files:\n",
    "    ent_vals = compute_entropy(f)\n",
    "    X_entropy_cats.append(ent_vals)\n",
    "\n",
    "X_entropy_cats = np.array(X_entropy_cats)\n",
    "print('Shape of entropy features for cats:', X_entropy_cats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0092735e",
   "metadata": {},
   "source": [
    "Since we are working with images, it makes sense to plot them for inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c09c92",
   "metadata": {
    "id": "94c09c92"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "# Select one image and compute its entropy\n",
    "sample_image = cat_files[0]  # Pick the first image\n",
    "entropy_map = compute_entropy(sample_image)\n",
    "\n",
    "# Reshape the entropy map back to 2D (assuming IMG_SIZE x IMG_SIZE)\n",
    "entropy_map_reshaped = entropy_map.reshape(IMG_SIZE, IMG_SIZE)\n",
    "\n",
    "# Load and resize the original image for display\n",
    "original_image = cv2.imread(sample_image)\n",
    "original_image = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "original_image_resized = cv2.resize(original_image, (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "# Plot original image and entropy map side by side\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Show original image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(original_image_resized)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Show entropy map\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(entropy_map_reshaped, cmap='inferno')\n",
    "plt.colorbar(label=\"Entropy\")\n",
    "plt.title(\"Entropy Map\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2830d8f7",
   "metadata": {
    "id": "2830d8f7"
   },
   "source": [
    "In practice, you might combine cats and dogs, then label them accordingly. Then, you could decide on an entropy threshold or a selection strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c6a7b3",
   "metadata": {
    "id": "98c6a7b3"
   },
   "source": [
    "### Dimensionality reduction with Autoencoders\n",
    "\n",
    "Autoencoders are a type of neural network that learn how to *compress* and then *rebuild* data. The goal is to take something complex — like an image — and turn it into a much simpler version that still keeps all the important information.\n",
    "\n",
    "The model has two main parts:\n",
    "\n",
    "- *Encoder*: This part squeezes the original image into a smaller, more compact form, removing unnecessary details and keeping only the most useful features.  \n",
    "- *Decoder*: This part tries to recreate the original image from that compact version as accurately as possible.\n",
    "\n",
    "During training, the model learns which parts of the image are essential and which parts can be ignored. Once trained, we can use just the encoder to reduce the size of our data. This gives us a set of features that are much smaller in size, but still meaningful.\n",
    "\n",
    "This process is known as *dimensionality reduction*. It works in a similar way to methods like PCA (Principal Component Analysis), but autoencoders are more powerful because they can learn complex, non-linear patterns.\n",
    "\n",
    "Below, we prepare the data and build the autoencoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6581c515",
   "metadata": {
    "id": "6581c515"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define parameters\n",
    "IMG_SIZE = 64  # Resize all images to 64x64\n",
    "BATCH_SIZE = 8  # Reduce batch size if dataset is small\n",
    "\n",
    "# Load Images from Directory\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Normalise images (Scale pixel values to [0,1]) and keep labels to prevent errors\n",
    "train_ds = train_ds.map(lambda x, _: (x / 255.0, x / 255.0))\n",
    "\n",
    "# Debug: Check if dataset is loading correctly\n",
    "for batch in train_ds.take(1):\n",
    "    print(\"Sample batch shape:\", batch[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be02832",
   "metadata": {},
   "source": [
    "The values returned as the image dataset shape e.g. `(8, 64, 64, 3)`, describe the structure of our processed image data in numerical terms. Here’s what each number means:\n",
    "\n",
    "- 8– The total number of images in the batch of training\n",
    "\n",
    "- 64 – The height of each image in pixels (after resizing)\n",
    "\n",
    "- 64 – The width of each image in pixels\n",
    "\n",
    "- 3 – The number of colour channels (Red, Green, Blue)\n",
    "\n",
    "So this tells us that we have 8 colour images in our batch, each resized to 64×64 pixels, ready for model training or further analysis.\n",
    "\n",
    "In our case, let's set up and train the autoencoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9872bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Autoencoder architecture\n",
    "# Set the input image shape (e.g. 128x128 pixels with 3 colour channels)\n",
    "input_shape = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Set the size of the encoded representation (latent space)\n",
    "encoding_dim = 256\n",
    "\n",
    "# Encoder: Compresses the input image into a smaller feature representation\n",
    "input_img = layers.Input(shape=input_shape)  # Input layer\n",
    "\n",
    "# First convolutional layer: 32 filters, ReLU activation\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "\n",
    "# Downsample the feature map (reduce spatial dimensions while keeping information)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# Second convolutional layer: 64 filters, ReLU activation\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "# Another downsampling step\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "# Final encoding layer: compress the feature maps into a more compact representation\n",
    "encoded = layers.Conv2D(encoding_dim, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "# Decoder: Reconstructs the original image from the encoded representation\n",
    "# First decoding layer: attempt to reconstruct the 64-filter feature map\n",
    "x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(encoded)\n",
    "\n",
    "# Upsample to increase spatial resolution\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "# Second decoding layer: return to 32 filters\n",
    "x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "\n",
    "# Final upsampling step to reach original size\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "\n",
    "# Output layer: 3 filters to match RGB image channels, sigmoid for pixel values between 0 and 1\n",
    "decoded = layers.Conv2D(3, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "# Compile the autoencoder model\n",
    "# Connect input and output through the defined layers\n",
    "autoencoder = models.Model(input_img, decoded)\n",
    "\n",
    "# Compile the model using the Adam optimiser and mean squared error (MSE) loss\n",
    "# MSE measures how close the reconstructed image is to the original\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Train the autoencoder\n",
    "# Fit the model to the training dataset (train_ds) for 10 epochs\n",
    "autoencoder.fit(train_ds, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2adee46",
   "metadata": {
    "id": "b2adee46"
   },
   "source": [
    "Once our model has finished training we can use it to create reconstructed version of the original image to use as features in simpler model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d500147c",
   "metadata": {
    "id": "d500147c"
   },
   "outputs": [],
   "source": [
    "# Get some test images from the dataset\n",
    "test_images, _ = next(iter(train_ds))  # Get a batch of test images\n",
    "\n",
    "# Pass them through the autoencoder\n",
    "reconstructed_images = autoencoder.predict(test_images)\n",
    "\n",
    "# Plot Original vs Reconstructed Images\n",
    "fig, axes = plt.subplots(2, BATCH_SIZE, figsize=(12, 4))  # 2 rows, 10 columns\n",
    "\n",
    "for i in range(min(BATCH_SIZE, test_images.shape[0])):  # Ensure we don't index out of range\n",
    "    # Original images\n",
    "    axes[0, i].imshow(test_images[i])\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Reconstructed images\n",
    "    axes[1, i].imshow(reconstructed_images[i])\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "# Add titles\n",
    "axes[0, 0].set_title('Original Images')\n",
    "axes[1, 0].set_title('Reconstructed Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56cbe0",
   "metadata": {
    "id": "0f56cbe0"
   },
   "source": [
    "### What have we learnt?\n",
    "\n",
    "When working with image data, choosing or extracting the right features can have a major impact on model performance and efficiency. Many classical feature selection methods do not directly account for the spatial structure of images, so more sophisticated approaches like CNN-based feature extraction, saliency maps, and deep learning–based dimensionality reduction are often more suitable.  In summary:\n",
    "\n",
    "- *Pre-Trained CNNs*: are a strong, practical baseline for feature extraction, making use of knowledge gained from large datasets.\n",
    "- *Saliency Maps and Grad-CAM*: methods help us visualise which parts of the image matter most for a model's classification, aiding interpretability and manual selection of relevant regions.\n",
    "- *Entropy*: can help locate highly variable regions in images, which might contain more discriminative information than homogeneous areas.\n",
    "- *Mutual Information*: allows us to rank each pixel's relevance to the class label, though it ignores spatial dependencies and can be computationally demanding.\n",
    "- *Autoencoders*: learn compressed representations tailored to the image data (particularly convolutional autoencoders), providing an alternative to more linear dimensionality reduction methods .\n",
    "\n",
    "In practice, one often combines or compares multiple approaches: for instance, extracting CNN-based features and then using a technique like mutual information or entropy to further refine which features are retained, or applying an autoencoder to reduce dimensionality and then training a relatively simple classifier on the lower-dimensional representation.\n",
    "\n",
    "Experimentation and careful assessment (e.g., via cross-validation) will guide which method (or combination of methods) provides the most reliable, accurate, and interpretable results for a specific image classification task."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
