{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cf92331",
   "metadata": {
    "id": "7cf92331"
   },
   "source": [
    "## Feature Selection – Language data\n",
    "### Introduction\n",
    "\n",
    "When working with text classification, selecting the most relevant words or phrases (features) is a crucial first step. Instead of treating all words equally, we can apply various methods to assess feature importance, ensuring that our model focuses on the most meaningful terms.\n",
    "\n",
    "Raw text data contains a large number of words, many of which may be irrelevant, redundant, or even misleading for classification.\n",
    "\n",
    "If we do not apply feature selection to our text data, models may become slower and less efficient, as they have to process too many features. In addition, irrelevant or frequent but uninformative words (e.g., \"the\", \"is\", \"and\") can dilute the signal needed for classification. This mmeans, certain rare but highly indicative words (e.g., \"refund\" in a complaint review) might get lost in a sea of common terms.\n",
    "\n",
    "This can also lead to overfitting if the model learns from noise rather than meaningful patterns. We can improve model performance while maintaining interpretability, by filtering and selecting only the most informative features.\n",
    "\n",
    "We will cover the most common feature selection techniques employed for language data, starting from the simplest approaches, before building up to more sophisticated methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mYG78btwZ-Ig",
   "metadata": {
    "id": "mYG78btwZ-Ig"
   },
   "source": [
    "### Installing Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GdwWok6xZ-P4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10788,
     "status": "ok",
     "timestamp": 1742233861702,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "GdwWok6xZ-P4",
    "outputId": "54114991-e570-4238-ec2c-6a86d481ab75"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "\n",
    "!pip install nltk pandas matplotlib wordcloud seaborn scikit-learn transformers torch gensim scapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a04d4d3",
   "metadata": {
    "id": "7a04d4d3"
   },
   "source": [
    "### Downloading the data\n",
    "We download the data and unzip it to a folder ready for use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7361dd1",
   "metadata": {
    "executionInfo": {
     "elapsed": 109796,
     "status": "ok",
     "timestamp": 1742233971499,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "e7361dd1"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# IMDb dataset URL. Uncomment to choose the larger version depending on your hardware\n",
    "# url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" # Size 80.2MB\n",
    "\n",
    "# Use an earlier version - smaller for demonstration\n",
    "url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_0211.tar.gz\" # Size 2.2MB\n",
    "\n",
    "# Download the dataset to the current directory\n",
    "urllib.request.urlretrieve(url, \"aclImdb_v1.tar.gz\") \n",
    "\n",
    "# Unpack (extract) the dataset\n",
    "with tarfile.open(\"aclImdb_v1.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6VmoBAtZz-Y",
   "metadata": {
    "id": "c6VmoBAtZz-Y"
   },
   "source": [
    "### Loading the data\n",
    "We will use the [Movie Review Dataset (Cornell)](http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_0211.tar.gz)\n",
    "\n",
    "We will assume our dataset is structured as follows, with our reviews split into two folders representing positive and negative sentiment:  \n",
    "\n",
    "```\n",
    "tokens/\n",
    "  ├── neg/\n",
    "  │    ├── cv000_tok-29416.txt\n",
    "  │    ├── cv001_tok-19502.txt\n",
    "  │    ...\n",
    "  └── pos/\n",
    "       ├── cv000_tok-29590.txt\n",
    "       ├── cv001_tok-18431.txt\n",
    "       ...\n",
    "```\n",
    "\n",
    "Here, the dataset consists of two labelled categories:  \n",
    "- *`pos/` (positive)* – Documents expressing positive sentiment.  \n",
    "- *`neg/` (negative)* – Documents expressing negative sentiment.  \n",
    "\n",
    "Before we start, we will prepare the features for a text classification task, specifically with two categories: *'pos'* (positive) and *'neg'* (negative). \n",
    "\n",
    "We start by creating two empty lists, *X* and *Y*, which will hold our text data and corresponding labels, respectively. We then loop through each category, assigning each category a numeric label (0 for positive, 1 for negative). \n",
    "\n",
    "Within each category, we open every text file in the category's folder, read its contents, and append this content directly to our list `X`. At the same time, we record the category's numeric label in the list `Y`. \n",
    "After the loop completes, we have a dataset of texts in `X` along with their respective labels in `Y`, making the data ready for training a machine learning model, such as one for sentiment analysis or document classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce99c28",
   "metadata": {
    "executionInfo": {
     "elapsed": 3063,
     "status": "ok",
     "timestamp": 1742233974564,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "8ce99c28"
   },
   "outputs": [],
   "source": [
    "# Define the two categories (folders): 'pos' (positive), 'neg' (negative)\n",
    "categories = ['pos', 'neg']\n",
    "\n",
    "# Initialise empty lists to store data (X) and labels (Y)\n",
    "X, Y = [], []\n",
    "\n",
    "# Loop through each category and its index\n",
    "for idx, cat in enumerate(categories):\n",
    "\n",
    "    # Loop through all files within the current category's folder\n",
    "    for filename in os.listdir(os.path.join(root_dir, cat)):\n",
    "        \n",
    "        # Open and read the content of each text file\n",
    "        with open(os.path.join(root_dir, cat, filename), 'r') as f:\n",
    "\n",
    "            # Add the text content to list X\n",
    "            X.append(f.read())\n",
    "\n",
    "            # Add the numeric category label (0 or 1) to list Y\n",
    "            Y.append(idx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KUqRACMMuIsl",
   "metadata": {
    "id": "KUqRACMMuIsl"
   },
   "source": [
    "We will take a sample of the data to speed things up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RgVONsutuHr9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1742233974580,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "RgVONsutuHr9",
    "outputId": "2bd4d3e6-bf22-47a7-ee81-01fd0ea24c24"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "n = 500  # Number of samples to take\n",
    "\n",
    "# Set the random state, so that we can reproduce the same sample each time we run the code\n",
    "seed = 7\n",
    "random.seed(seed)\n",
    "\n",
    "# Take n samples\n",
    "sample_indices = random.sample(range(len(X)), n)\n",
    "\n",
    "# Take a sample of the full dataset and overwrite the lists\n",
    "X = [X[i] for i in sample_indices]\n",
    "Y = [Y[i] for i in sample_indices]\n",
    "\n",
    "# Convert Y to integer type\n",
    "Y = np.array(Y, dtype=int)\n",
    "\n",
    "# Inspect 5 of the samples\n",
    "top_n = 5\n",
    "for i in range(5):\n",
    "    print(f\"Review {i+1}: {X[i][:100]}...\")  # Print first 200 chars for brevity\n",
    "    print(f\"Sentiment: {Y[i]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agdXmc-EpkRG",
   "metadata": {
    "id": "agdXmc-EpkRG"
   },
   "source": [
    "## Domain-specific and manual selection\n",
    "These methods assess feature importance using mathematical criteria. Domain-specific and manual selection includes, many methods we have covered before:\n",
    "- *Stopword removal*: eliminating common words.\n",
    "- *N-gram filtering*: selecting meaningful unigrams, bigrams, or trigrams.  \n",
    "- *POS-based selection*: focusing on key content words like nouns and verbs.\n",
    "- *Named Entity Recognition (NER)*: extracting entities like names, locations, and organisations to enhance interpretability.\n",
    "\n",
    "Let's recap on how we can use them for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XujYD2vurZy2",
   "metadata": {
    "id": "XujYD2vurZy2"
   },
   "source": [
    "#### Stop word removal\n",
    "Many words appear frequently in all texts but carry little meaning on their own. For example, words like \"the\", \"is\", \"and\", and \"it\" occur in almost every sentence but do not help distinguish categories.\n",
    "\n",
    "Stopwords can dilute important patterns in text data and increase computational complexity. We can reduce noise and focus on more meaningful words that contribute to classification by removing them.\n",
    "\n",
    "Before stopword removal:\n",
    "```\n",
    "\"The movie was really amazing, and it had a fantastic plot!\"\n",
    "```\n",
    "After stopword removal:\n",
    "```\n",
    "\"Movie amazing fantastic plot!\"\n",
    "```\n",
    "For our dataset, this helps models focus on important words like \"amazing\", \"fantastic\", and \"plot\", which indicate sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "joYwlHHoqHcv",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21191,
     "status": "ok",
     "timestamp": 1742233995772,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "joYwlHHoqHcv",
    "outputId": "f51437cc-eee1-4514-cfea-03a2e4c6242c"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download stopwords if not already available\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "# Function to remove stopwords from reviews\n",
    "def remove_stopwords(text):\n",
    "    words = word_tokenize(text.lower())  # Convert to lowercase and tokenize\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)  # Reconstruct sentence\n",
    "\n",
    "# Apply stopword removal to all reviews in X\n",
    "X_clean = [remove_stopwords(review) for review in X]\n",
    "\n",
    "# Display cleaned reviews\n",
    "top_n = 10 # Show the first ten:\n",
    "for i, (original, cleaned) in enumerate(zip(X, X_clean)):\n",
    "    if i >=top_n: break\n",
    "    print(f\"Original: {original}\\nCleaned: {cleaned}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5b02rztbZM",
   "metadata": {
    "id": "6c5b02rztbZM"
   },
   "source": [
    "#### Part Of Speech (POS)-based selection:\n",
    "Part-of-Speech (POS) tagging allows us to filter words based on grammatical categories. Not all words contribute equally to classification, so we can focus on those with more semmantic meaning:\n",
    "\n",
    "- Nouns (e.g., \"election\", \"policy\", \"economy\") are important for tasks such as topic classification.\n",
    "- Verbs (e.g., \"win\", \"increase\", \"fail\") are useful for sentiment analysis or event detection.\n",
    "- Adjectives & Adverbs (e.g., \"terrible\", \"wonderfully\") often strong indicators in sentiment analysis.\n",
    "\n",
    "As an example, consider the following review:\n",
    "\n",
    "     \"The [food] was absolutely [terrible], and the [service] was [slow].\"\n",
    "\n",
    "POS-based selection keeps words like \"food\", \"terrible\", \"service\", and \"slow\", as they are strong indicators of sentiment, and any model we train will learn more efficiently if we filter out these less meaningful words (e.g., determiners, conjunctions).\n",
    "\n",
    "Part of speech taggers may use more than one tag label. For nouns we have the following breakdown:\n",
    "\n",
    "- NN: Singular noun (e.g., dog, table, car)\n",
    "- NNS: Plural noun (e.g., dogs, tables, cars)\n",
    "- NNP: Proper noun, singular (e.g., John, London, Apple)\n",
    "- NNPS: Proper noun, plural (e.g., Americans, Beatles)\n",
    "\n",
    "Therefore, to capture all instances of a particular part-of-speech, we can filter for the first two characters of the POS-tag to capture them all:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nh2gB0xUte0U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1078,
     "status": "ok",
     "timestamp": 1742233996849,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "Nh2gB0xUte0U",
    "outputId": "7c74d98c-1556-47d1-c1c7-6b9d0c67d556"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download necessary NLTK models\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Example review - pick the first from our training data\n",
    "example_review = X[0]\n",
    "\n",
    "# Tokenize the words\n",
    "words = nltk.word_tokenize(example_review)\n",
    "\n",
    "# Perform POS tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "\n",
    "# Extract NOUNS (NN*), VERBS (VB*), and ADJECTIVES (JJ*)\n",
    "nouns = [word for word, pos in pos_tags if pos.startswith('NN')]\n",
    "verbs = [word for word, pos in pos_tags if pos.startswith('VB')]\n",
    "adjectives = [word for word, pos in pos_tags if pos.startswith('JJ')]\n",
    "\n",
    "# Display results\n",
    "print(\"Original Sentence:\", example_review)\n",
    "print(\"\\nNOUNS:\", nouns)\n",
    "print(\"VERBS:\", verbs)\n",
    "print(\"ADJECTIVES:\", adjectives)\n",
    "print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hG0lJjPKshvW",
   "metadata": {
    "id": "hG0lJjPKshvW"
   },
   "source": [
    "#### N-gram filtering\n",
    "\n",
    "Words do not always hold meaning in isolation. Sometimes, phrases (n-grams) provide more context than single words (unigrams).  Unigrams (single words) can be useful but might lack context. However, bigrams (two-word phrases) and trigrams (three-word phrases) often capture more meaningful relationships:\n",
    "\n",
    "- Unigram: \"bank\", \"account\", \"fraud\"\n",
    "- Bigram: \"bank fraud\", \"fraud detection\"\n",
    "- Trigram: \"bank account fraud\", \"credit card scam\"\n",
    "\n",
    "In spam detection, for instance, bigrams like \"free money\" or \"limited offer\" are stronger indicators of spam than just \"free\" or \"money\" alone. However, using too many n-grams can introduce redundancy, so n-gram filtering helps retain only the most useful ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SKmU6EWgsjL1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2130,
     "status": "ok",
     "timestamp": 1742233998981,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "SKmU6EWgsjL1",
    "outputId": "edf86487-d785-4390-d2df-a751a05bc11c"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "ngram_size = 3\n",
    "\n",
    "# Extract unigrams (1) bigrams (2), and trigrams (3)\n",
    "vectoriser = CountVectorizer(ngram_range=(1, ngram_size))\n",
    "\n",
    "X_count = vectoriser.fit_transform(X)\n",
    "\n",
    "print(\"N-grams:\", vectoriser.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5XM0S_iqsn_t",
   "metadata": {
    "id": "5XM0S_iqsn_t"
   },
   "source": [
    "#### Named Entity Recognition (NER)\n",
    "Named Entity Recognition (NER) identifies important real-world entities that improve interpretability.  This involves extracting entities like names, locations, and organisations to improve interpretability and relevance. Some words carry specific meaning beyond their literal definition.\n",
    "\n",
    "For example:\n",
    "\n",
    "- People (e.g., \"Elon Musk\", \"Shakespeare\")\n",
    "- Locations (e.g., \"London\", \"New York\")\n",
    "- Organisations (e.g., \"Google\", \"United Nations\")\n",
    "- Dates & Events (e.g., \"Brexit\", \"World War II\")\n",
    "\n",
    "In news classification, recognising entities helps distinguish between topics.\n",
    "Articles mentioning \"NASA\" and \"Mars Rover\" likely belong to the \"science\" category. Articles mentioning \"vote\" and \"Democrats\" are probably about \"politics\".\n",
    "\n",
    "\n",
    "NER enhances interpretability, making models not just more accurate but also more understandable. Just like POS-tagging, we can label each word, or phrase, representing these entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "uoFrHz4It3ws",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10207,
     "status": "ok",
     "timestamp": 1742234009195,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "uoFrHz4It3ws",
    "outputId": "0edd08cb-b3a7-4b05-cd3a-6c2b8f146fcb"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Function to extract named entities\n",
    "def extract_named_entities(text):\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]  # Extract entity text and label\n",
    "\n",
    "# Apply NER to a sample of our reviews\n",
    "top_n = 10\n",
    "X_named_entities = [extract_named_entities(review) for review in X[:top_n]]\n",
    "\n",
    "# Display results\n",
    "for i, (review, entities) in enumerate(zip(X, X_named_entities)):\n",
    "    print(f\"Review {i+1}: {review}\")\n",
    "    print(f\"Named Entities: {entities}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-x9svmNTabFf",
   "metadata": {
    "id": "-x9svmNTabFf"
   },
   "source": [
    "## Statistical methods\n",
    "In text classification tasks, we often want to understand how different words contribute to distinguishing between categories. One way to do this is by measuring the correlation between words and labels.  \n",
    "\n",
    "For example, consider a dataset of customer reviews, where each review is labelled as \"positive\" or \"negative\". Some words, like \"excellent\" and \"amazing\", are likely to appear more frequently in positive reviews, while words like \"terrible\" and \"disappointing\" may be more common in negative reviews.  \n",
    "\n",
    "If we analyse how often certain words appear in each category compared to others, we can identify words that strongly differentiate one category from another.  We will cover different ways we can measure the relationship between words and labels to figure out which words act as clues for different categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "UOmctObBlM15",
   "metadata": {
    "id": "UOmctObBlM15"
   },
   "source": [
    "#### Term Frequency-Inverse Document Frequency (TF-IDF):\n",
    "TF-IDF measures how important a word is in a document relative to its frequency across all documents. It assigns higher scores to words that appear frequently in a document but not across the entire dataset, reducing the impact of common words.\n",
    "\n",
    "This approach helps filter out very frequent but uninformative words (e.g., \"the\", \"is\") while keeping meaningful words. It also highlights certain domain-specific words that appear in fewer documents, but are certainly indicative of the topic (e.g., \"refund\" in complaint reviews) and should be retained.\n",
    "\n",
    "This method does not capture the relationship between words, nor the context or meaning of words. In addition, rare words may get overemphasised, even if they are not important for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hlJzIwcdkPFq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 114,
     "status": "ok",
     "timestamp": 1742234009314,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "hlJzIwcdkPFq",
    "outputId": "29904cab-ba42-4489-cd88-50d15e35c1a1"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectoriser = TfidfVectorizer()\n",
    "\n",
    "X_tfidf = vectoriser.fit_transform(X)\n",
    "\n",
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TU3hlKQziMjC",
   "metadata": {
    "id": "TU3hlKQziMjC"
   },
   "source": [
    "The output `(0, 3080)\t0.15187519523325435` represents (row, column, TF-IDF score), which is not human-readable. The statement `TfidfVectorizer.fit_transform(X)` returns a sparse matrix (`scipy.sparse.csr_matrix`). This sparse matrix stores only non-zero values to save memory, since most entries in TF-IDF matrices are zero (i.e., words that do not appear in a document get a 0 score).\n",
    "\n",
    "If the dataset is small, converting to a dense matrix makes it easier to work with and visualise. We use `.toarray()` to convert the sparse matrix into a NumPy array so we can view all values at once. If your dataset is large, however, keeping it in sparse format is more memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11aUSlbac6pL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 118,
     "status": "ok",
     "timestamp": 1742234009433,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "11aUSlbac6pL",
    "outputId": "5a235e02-b171-444d-f964-bfcc1a6df445"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert sparse matrix to dense\n",
    "X_tfidf_dense = X_tfidf.toarray()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectoriser.get_feature_names_out()\n",
    "\n",
    "# Create a DataFrame for better visualisation\n",
    "df_tfidf = pd.DataFrame(X_tfidf_dense, columns=feature_names)\n",
    "\n",
    "# Select some words and display the results\n",
    "df_tfidf[['movie', 'film', 'great', 'bad']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YMYd4UPBlRvT",
   "metadata": {
    "id": "YMYd4UPBlRvT"
   },
   "source": [
    "#### Chi-Square test\n",
    "The chi-square test measures how strongly a word’s occurrence is associated with a particular category compared to what would be expected by chance. It’s a statistical test often used for feature selection in text classification.\n",
    "\n",
    "We can use it to help select words that have a strong association with specific labels, improving classification accuracy. It works well for categorical data where words need to be linked to discrete categories. And it also provides an interpretable way to rank features based on their relevance to classification.\n",
    "\n",
    "Again, it assumes assumes independence between words, which does not always hold true in natural language, since words appear in a relationship with other words due to the syntax (or grammar) of the language. It can also be Less effective for rare words, as low-frequency terms may not show significant statistical association."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hMhFlSRHdtmu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 708
    },
    "executionInfo": {
     "elapsed": 578,
     "status": "ok",
     "timestamp": 1742234010012,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "hMhFlSRHdtmu",
    "outputId": "eacf3f71-e613-49af-a037-ed4147b834af"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorise text\n",
    "vectoriser = TfidfVectorizer()\n",
    "X_tfidf = vectoriser.fit_transform(X)  # Use the fitted vectoriser\n",
    "\n",
    "# Compute Chi-Square scores\n",
    "chi2_scores, _ = chi2(X_tfidf, Y)\n",
    "\n",
    "chi2_scores = np.array(chi2_scores).flatten()  # Ensure correct shape\n",
    "\n",
    "feature_names = vectoriser.get_feature_names_out()\n",
    "\n",
    "# Select top 10 features\n",
    "top_n = 10\n",
    "top_features = np.argsort(chi2_scores)[-top_n:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762c31fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top features\n",
    "print(\"Top features by Chi-Square score:\")\n",
    "for i in reversed(top_features):\n",
    "    print(f\"{feature_names[i]}: {chi2_scores[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62603d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top features\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.barh([feature_names[i] for i in top_features], [chi2_scores[i] for i in top_features], color='blue')\n",
    "\n",
    "plt.xlabel(\"Chi-Square Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "\n",
    "plt.title(\"Top features by Chi-Square Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5EHG1hcPoV7Y",
   "metadata": {
    "id": "5EHG1hcPoV7Y"
   },
   "source": [
    "#### Mutual Information\n",
    "Mutual Information (MI) measures how much knowing the presence (or absence) of a word helps predict a category. It calculates how much uncertainty is reduced when we observe a particular word in a document.\n",
    "\n",
    "This approach allows us to identify words that are most informative for classification. It works well with imbalanced datasets, as it doesn’t rely on absolute word counts. In addition, it can handle both presence/absence and frequency-based word representations.\n",
    "\n",
    "However, like chi-square, it can overemphasise rare words, which might not always be meaningful. And it doesn’t consider relationships between words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "teqU8pcuoWG4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 487
    },
    "executionInfo": {
     "elapsed": 27451,
     "status": "ok",
     "timestamp": 1742234037464,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "teqU8pcuoWG4",
    "outputId": "a9f1ebf4-fb92-4762-de1a-4d82df567cd4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "\n",
    "# Vectorise text data\n",
    "vectoriser = TfidfVectorizer()\n",
    "X_tfidf = vectoriser.fit_transform(X)\n",
    "\n",
    "# Convert sparse matrix to dense array\n",
    "X_dense = X_tfidf.toarray()  # Convert to dense\n",
    "\n",
    "# Compute Mutual Information scores\n",
    "mi_scores = mutual_info_classif(X_dense, Y, discrete_features=False)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectoriser.get_feature_names_out()\n",
    "\n",
    "# Sort and get top 10 features by MI score\n",
    "top_mi_features = sorted(zip(feature_names, mi_scores), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "# Extract words and scores\n",
    "top_words, top_scores = zip(*top_mi_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc75fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot top features\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.barh(top_words, top_scores, color='blue')\n",
    "\n",
    "plt.xlabel(\"MI Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "\n",
    "plt.title(\"Top features by MI Score\")\n",
    "\n",
    "plt.gca().invert_yaxis()  # Invert Y-axis for better visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KwosbQxCdtum",
   "metadata": {
    "id": "KwosbQxCdtum"
   },
   "source": [
    "## Dimensionality reduction techniques\n",
    "\n",
    "When working with text data, we often represent words and documents as numerical features in a very high-dimensional space. For example, if we have 10,000 unique words in a dataset, each document is represented as a vector with 10,000 dimensions. This creates several challenges, including:\n",
    "\n",
    "- Computational inefficiency – More features mean longer processing times and higher memory usage.\n",
    "- Redundancy – Many words convey similar meanings, leading to overlapping information.\n",
    "- Overfitting risk – Too many features can make models learn noise instead of meaningful patterns.\n",
    "\n",
    "To solve these issues, dimensionality reduction techniques transform high-dimensional word vectors into a smaller, more manageable space while preserving the most important information. This helps models run faster, generalise better, and improve interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ol5yaZcMRbtC",
   "metadata": {
    "id": "Ol5yaZcMRbtC"
   },
   "source": [
    "### Latent Semantic Analysis (LSA)\n",
    "Latent Semantic Analysis (LSA) is a dimensionality reduction technique used to find hidden relationships between words and documents. It helps transform high-dimensional text data into a lower-dimensional representation, making it easier to analyse while preserving key patterns in word usage.\n",
    "\n",
    "LSA is based on a mathematical technique called Singular Value Decomposition (SVD), which breaks down large document-term matrices into smaller, more manageable ones while retaining the most important information about word meanings.\n",
    "\n",
    "In text data, there are several challenges:\n",
    "- *Synonyms and variations*: Different words can have the same meaning (e.g., \"car\" vs \"automobile\"). Traditional methods treat these as separate words, but LSA groups them together.\n",
    "- *High dimensionality*: Large vocabularies create thousands of features (words), making models slow and complex. LSA reduces the number of dimensions while keeping key information.\n",
    "- *Noise reduction*: Text data has redundant or irrelevant words. LSA helps remove unimportant words while keeping meaningful structure.\n",
    "\n",
    "The process begins by converting text into a document-term matrix, where each document is represented as a vector of word frequencies or TF-IDF values. This matrix is then decomposed using Singular Value Decomposition (SVD), breaking it into three smaller matrices: *U*, which represents documents in a reduced space; *𝑆*, which contains singular values indicating the importance of each concept; and *𝑉*, which represents words in the same reduced space.\n",
    "\n",
    "To enhance efficiency and remove noise, LSA retains only the most important concepts, discarding smaller singular values that contribute less to meaning. This results in a transformed representation where both documents and words are mapped to a lower-dimensional space that captures their latent relationships.\n",
    "\n",
    "There are disadvantages, which include a loss of Interpretability as the reduced dimensions are abstract, making it hard to explain what each new feature represents. It can be computationally expensive since computing SVD on large datasets requires significant processing power. Like some of these other methods, it ignores word order and focuses only on word co-occurrence, but doesn’t consider how words are arranged in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eGpinPfRb3S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 431,
     "status": "ok",
     "timestamp": 1742234037894,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "4eGpinPfRb3S",
    "outputId": "74fce052-588c-4465-a32a-d805ae87bc1e"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorise text using TF-IDF\n",
    "vectoriser = TfidfVectorizer(stop_words=\"english\", max_features=5000)  # Limit features to 5000 for efficiency\n",
    "X_tfidf = vectoriser.fit_transform(X)\n",
    "\n",
    "seed = 7\n",
    "\n",
    "# Apply LSA (Truncated SVD for text data)\n",
    "lsa = TruncatedSVD(n_components=2, random_state=seed)\n",
    "X_lsa = lsa.fit_transform(X_tfidf)\n",
    "\n",
    "# Scatter plot with corrected legend\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Define colors and labels correctly\n",
    "for sentiment, color, label in zip([1, 0], [\"blue\", \"red\"], [\"Positive Reviews\", \"Negative Reviews\"]):\n",
    "    indices = [i for i, lbl in enumerate(Y) if lbl == sentiment]\n",
    "    \n",
    "    plt.scatter(X_lsa[indices, 0], X_lsa[indices, 1], c=color, alpha=0.7, label=label)\n",
    "\n",
    "plt.xlabel(\"LSA Component 1\")\n",
    "plt.ylabel(\"LSA Component 2\")\n",
    "\n",
    "plt.title(\"LSA Projection of movie reviews\")\n",
    "\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1YHwAR-wKxhz",
   "metadata": {
    "id": "1YHwAR-wKxhz"
   },
   "source": [
    "The plot represents movie reviews transformed into a 2D space using Latent Semantic Analysis (LSA). Each point corresponds to a review, with:\n",
    "\n",
    "- Blue points representing positive reviews\n",
    "- Red points representing negative reviews\n",
    "\n",
    "The x-axis (LSA Component 1) and y-axis (LSA Component 2) capturing the most important semantic patterns in the dataset.\n",
    "\n",
    "Originally, each review had hundreds of word features (from TF-IDF).\n",
    "LSA compressed this into just two components while preserving key relationships.\n",
    "\n",
    "Many positive reviews (blue) and negative reviews (red) appear in different regions, suggesting that LSA captures sentiment-related differences. However, there is some overlap, indicating that some reviews might be harder to distinguish based on LSA alone.\n",
    "\n",
    "If LSA can separate positive and negative reviews effectively, we can use these components as features for our machine learning models (e.g., logistic regression, SVM) to classify new movie reviews based on sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3F5SGamoRoUx",
   "metadata": {
    "id": "3F5SGamoRoUx"
   },
   "source": [
    "### Non-Negative Matrix Factorisation (NMF)\n",
    "Non-Negative Matrix Factorisation (NMF) is a dimensionality reduction technique that helps break down complex datasets into meaningful components while ensuring that all values remain non-negative (i.e., no negative numbers).\n",
    "\n",
    "Think of it like finding hidden topics in a collection of documents by identifying important word groupings. Unlike other techniques (like PCA or LSA), NMF ensures that all components make sense in human terms because it doesn’t mix negative and positive numbers.\n",
    "\n",
    "Many dimensionality reduction techniques, create components where some words have positive weights and others have negative weights. This can make interpretation difficult because negative values do not have a clear meaning in human language. Since all values are positive, there is no ambiguity in meaning — each word adds to the topic.\n",
    "\n",
    "NMF begins by converting text into a *document-term* matrix, where each document is represented as a vector of word frequencies or *TF-IDF* values.\n",
    "\n",
    "The matrix is then factorised into two smaller matrices:\n",
    "\n",
    "- *W* (Document-Topic matrix), which indicates how strongly each document relates to different topics, and\n",
    "- *H* (Topic-Word matrix), which captures how much each word contributes to different topics.\n",
    "\n",
    "The final step is to interpret the topics by identifying patterns of words that frequently appear together, revealing the hidden structure within the dataset.\n",
    "\n",
    "This process helps uncover meaningful word groupings, making it useful for topic modelling, sentiment analysis, and document classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RcczF5X6Rp15",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 46,
     "status": "ok",
     "timestamp": 1742234037940,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "RcczF5X6Rp15",
    "outputId": "3f7e70d5-4723-4ce3-b840-4f66da7906d4"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Apply NMF for dimensionality reduction\n",
    "seed = 7\n",
    "nmf = NMF(n_components=2, init=\"nndsvd\", random_state=seed)\n",
    "\n",
    "X_nmf = nmf.fit_transform(X_tfidf)\n",
    "\n",
    "print(X_nmf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "P2TgFU0xObZK",
   "metadata": {
    "id": "P2TgFU0xObZK"
   },
   "source": [
    "Here is a bar chart displaying the top words for each NMF component. Each bar represents a word that contributes significantly to a particular NMF component (topic). Higher bars indicate words that are more strongly associated with the component. We can see distinct word groupings, which can help interpret what each component represents.\n",
    "\n",
    "If a component contains words like \"great\", \"amazing\", \"fantastic\", it likely represents positive sentiment. If a component includes words like \"terrible\", \"awful\", \"disappointing\", it likely represents negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vMlOsicwObmS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 372,
     "status": "ok",
     "timestamp": 1742234038313,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "vMlOsicwObmS",
    "outputId": "18aba0b9-a129-419e-bf5a-6ead7499fbd0"
   },
   "outputs": [],
   "source": [
    "# Extract top words for each NMF component (topic)\n",
    "num_top_words = 10\n",
    "\n",
    "feature_names = vectoriser.get_feature_names_out()\n",
    "\n",
    "# Get the most important words for each component\n",
    "top_words_per_component = {}\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[:-num_top_words - 1:-1]]\n",
    "    top_words_per_component[f\"Component {topic_idx + 1}\"] = top_words\n",
    "\n",
    "# Convert to DataFrame for easier visualisation\n",
    "df_top_words = pd.DataFrame(top_words_per_component)\n",
    "\n",
    "# Plot top words per NMF component\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, column in enumerate(df_top_words.columns):\n",
    "    plt.barh(df_top_words[column], nmf.components_[i, topic.argsort()[:-num_top_words - 1:-1]])\n",
    "\n",
    "plt.xlabel(\"Weight\")\n",
    "plt.ylabel(\"Words\")\n",
    "\n",
    "plt.title(\"Top words in each NMF component\")\n",
    "\n",
    "plt.legend(df_top_words.columns)\n",
    "\n",
    "# Rotate the plot axes to make it easier to interpret\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "A_P-ut7MduEm",
   "metadata": {
    "id": "A_P-ut7MduEm"
   },
   "source": [
    "## Embedded Methods (Model-based selection)\n",
    "When working with text classification, we often deal with high-dimensional data, where each word becomes a feature. However, not all words are equally important—some contribute significantly to classification, while others add noise. Embedded methods help by automatically selecting the most relevant words while training the model.\n",
    "\n",
    "We will provide a few examples of model-based selection, starting with Lasso (L1 Regression), before moving to Random Forest-based approaches:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FlDCLckyc0dt",
   "metadata": {
    "id": "FlDCLckyc0dt"
   },
   "source": [
    "#### Lasso (L1 Regression)\n",
    "Lasso Regression is a machine learning technique that automatically selects the most important features (in this case, words) while removing irrelevant ones.\n",
    "\n",
    "It does this by adding a penalty (L1 regularisation) to the model, which forces it to shrink the impact of less useful words to zero. This means that only the most important words stay in the model, while unimportant words are removed (e.g. coefficient = 0). Regularisation prevents the model from becoming too complex by penalising large coefficients. This helps in reducing overfitting as it stops the model from memorising noise.\n",
    "\n",
    "The parameter to the model (see `C` below) is the inverse of regularisation strength, meaning a high `C` (e.g., C=10) provides weak regularisation meaning the model keeps more words (including less useful ones). A low value for `C` (e.g., C=0.1) provides strong regularisation meaning greater feature selection (fewer but stronger words remain). In some instances, this can lead to underfitting.\n",
    "\n",
    "We first start by selecting all words. Each word in the dataset is given a number (weight) that tells us how important it is. The model checks which words are really necessary and if a word isn’t very useful for classification, Lasso sets its importance to zero, effectively removing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kbS7yzQPduU1",
   "metadata": {
    "executionInfo": {
     "elapsed": 78,
     "status": "ok",
     "timestamp": 1742234038391,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "kbS7yzQPduU1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Vectorise text using TF-IDF\n",
    "vectoriser = TfidfVectorizer(stop_words=\"english\", max_features=10000)  # Limit features for efficiency\n",
    "X_tfidf = vectoriser.fit_transform(X)\n",
    "\n",
    "# Train a logistic regression model with L1 regularisation (Lasso)\n",
    "seed = 7\n",
    "lasso_model = LogisticRegression(penalty=\"l1\", solver=\"liblinear\", C=2.0, random_state=seed)\n",
    "lasso_model.fit(X_tfidf, Y)\n",
    "\n",
    "# Get feature importance (absolute value of coefficients)\n",
    "word_importance = np.abs(lasso_model.coef_).ravel()\n",
    "\n",
    "# Get feature names\n",
    "feature_names = vectoriser.get_feature_names_out()\n",
    "\n",
    "# Get indices of non-zero coefficients (important words)\n",
    "important_word_indices = np.where(word_importance > 0)[0]\n",
    "\n",
    "# Get indices of zero coefficients (unimportant words removed by Lasso)\n",
    "unimportant_word_indices = np.where(word_importance == 0)[0]\n",
    "\n",
    "# Extract important and unimportant words\n",
    "important_words = [feature_names[i] for i in important_word_indices]\n",
    "unimportant_words = [feature_names[i] for i in unimportant_word_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8_9L_c0wdsx8",
   "metadata": {
    "id": "8_9L_c0wdsx8"
   },
   "source": [
    "Let's visualise the results to easily identify the most important words and their importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4n5kSXaXmV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 619
    },
    "executionInfo": {
     "elapsed": 266,
     "status": "ok",
     "timestamp": 1742234038659,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "2a4n5kSXaXmV",
    "outputId": "44c9c35a-61d3-4a5a-dfd6-b9fe31d6015a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot Important Words (Top 10 by absolute Coefficient value)\n",
    "top_n = 10  # Limit the number of words for display\n",
    "important_word_values = word_importance[important_word_indices]\n",
    "top_indices = important_word_values.argsort()[-top_n:][::-1]  # Get top N important words\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.barh([important_words[i] for i in top_indices], [important_word_values[i] for i in top_indices], color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"Importance Score (Absolute Coefficient)\")\n",
    "plt.ylabel(\"Words\")\n",
    "\n",
    "plt.title(\"Top 10 important words selected by Lasso\")\n",
    "\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
    "plt.show()\n",
    "# Extract the unimportant words (random sample)\n",
    "num_unimportant = min(10, len(unimportant_words))  # Show up to 10 words\n",
    "sample_unimportant_words = np.random.choice(unimportant_words, num_unimportant, replace=False)\n",
    "\n",
    "print(\"Unimportant words - those removed by Lasso\")\n",
    "print(sample_unimportant_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ihz94pT7d1Tc",
   "metadata": {
    "id": "Ihz94pT7d1Tc"
   },
   "source": [
    "As we can see from the plot, Lasso automatically picks the most important words and we can see how they may relate to sentiment.\n",
    "\n",
    "An issue with this approach is that if the penalty `C` is too strong, it may remove too many words and miss useful information, so the appropriate value for `C` does require some experimentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "S4c4YIdad_09",
   "metadata": {
    "id": "S4c4YIdad_09"
   },
   "source": [
    "### Random Forest-based feature selection\n",
    "Imagine you’re trying to decide whether a movie review is positive or negative, but instead of making the decision alone, you ask a group of friends for their opinions. Some might focus on keywords like “fantastic” or “terrible,” while others consider different phrases. After gathering everyone’s input, you go with the majority vote. This is the how a Random Forest works.\n",
    "\n",
    "Random Forest is a machine learning algorithm that makes predictions by combining the results of multiple decision trees. Instead of relying on just one tree, it creates a \"forest\" of many trees, each trained on a different part of the data. Each tree gives its own prediction, and the final result is determined by a majority vote (for classification) or an average (for regression). This approach helps improve accuracy and makes the model more resistant to errors or noise in the data.\n",
    "\n",
    "One of the key benefits of Random Forest is its ability to rank feature importance, meaning it can tell us which words are the most useful for classification tasks like sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DG1z_JGweBY1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "executionInfo": {
     "elapsed": 648,
     "status": "ok",
     "timestamp": 1742234039752,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "DG1z_JGweBY1",
    "outputId": "ed1e9252-c873-409b-80bb-e0272cd6fcdd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Vectorise text using TF-IDF\n",
    "vectoriser = TfidfVectorizer(stop_words=\"english\", max_features=10000)  # Limit features for efficiency\n",
    "X_tfidf = vectoriser.fit_transform(X)\n",
    "\n",
    "seed = 7\n",
    "\n",
    "# Train Random Forest model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=seed)\n",
    "rf_model.fit(X_tfidf, Y)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance = rf_model.feature_importances_\n",
    "feature_names = vectoriser.get_feature_names_out()\n",
    "\n",
    "# Select top N important features\n",
    "top_n = 10\n",
    "top_indices = np.argsort(feature_importance)[-top_n:][::-1]  # Get top N important words\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.barh([feature_names[i] for i in top_indices], [feature_importance[i] for i in top_indices], color=\"blue\")\n",
    "\n",
    "plt.xlabel(\"Feature importance score\")\n",
    "plt.ylabel(\"Words\")\n",
    "\n",
    "plt.title(\"Top words in Random Forest Model\")\n",
    "\n",
    "plt.gca().invert_yaxis()  # Invert y-axis for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1PXDmqTCeDit",
   "metadata": {
    "id": "1PXDmqTCeDit"
   },
   "source": [
    "### Heuristic and Neural Network-based approaches\n",
    "\n",
    "Word Embeddings, Attention Mechanisms, and Autoencoders—can be used for feature selection and dimensionality reduction in text classification. Each method captures semantic meaning beyond simple word frequency. These are more advanced techniques that leverage deep learning to identify the semantics of a collection of texts (in our case reviews):\n",
    "\n",
    "- *Word Embeddings (Word2Vec, GloVe, FastText)*: Converts words into dense vectors, allowing for semantic feature selection.\n",
    "- *Attention Mechanisms*: In transformer-based models (e.g., BERT, GPT), attention scores indicate important words for prediction.\n",
    "- *Autoencoders*: Neural networks that learn compressed representations of text data, filtering out noise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DrmsrLnO7aYL",
   "metadata": {
    "id": "DrmsrLnO7aYL"
   },
   "source": [
    "#### Word Embeddings\n",
    "Word embeddings convert words into dense vectors, capturing semantic relationships (e.g., \"king\" and \"queen\" are considered similar in meaning).\n",
    "Unlike TF-IDF, which treats words independently, embeddings group similar words together in a lower-dimensional space.\n",
    "\n",
    "We can use it for feature selection to identify important words based on meaning, not just frequency. It can also be used to reduce the dimensions of our feature space as words with similar meanings have similar vectors.\n",
    "\n",
    "We create a model to learn word vectors, which can be used as features in our classification models. We can remove low-importance words by setting a minimum similarity threshold between word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F55FuYYuiiaa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1164,
     "status": "ok",
     "timestamp": 1742234040915,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "F55FuYYuiiaa",
    "outputId": "16ccfee1-7646-4456-d12a-3fcd7d8ad37f"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=X, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "word_embeddings = {word: model.wv[word] for word in model.wv.index_to_key}\n",
    "\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "G5lW1Mlcj1gR",
   "metadata": {
    "id": "G5lW1Mlcj1gR"
   },
   "source": [
    "We can visualise the results using t-SNE (t-Distributed Stochastic Neighbor Embedding), which is a machine learning algorithm used for visualising high-dimensional data in a lower-dimensional space, typically 2D or 3D. It helps identify patterns, relationships, and clusters in complex datasets, especially in text, images, and word embeddings.\n",
    "\n",
    "When using t-SNE (TSNE), we adjust parameters like `n_components`, and `perplexity` to control how word embeddings are visualised. t-SNE reduces high-dimensional embeddings (like TF-IDF, Word2Vec) into 2D coordinates, allowing us to plot them. So, setting `n_components=2`, allows us to reduce to 2D for visualisation.  This is because word embeddings exist in hundreds of dimensions, making them hard to visualise. Lowering to 2D helps us interpret relationships between words more easily:\n",
    "\n",
    "t-SNE balances local and global word relationships using the `perplexity` parameter. High perplexity (30-50) captures global structures (broad relationships). Whereas, low perplexity (3-5) Focuses on local clusters (similar words grouped together)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zFdCTbB8j1sR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1870,
     "status": "ok",
     "timestamp": 1742234042787,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "zFdCTbB8j1sR",
    "outputId": "a4e61b60-1c53-4a77-f838-84cfaeb935a6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(sentences=X, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "word_embeddings = {word: model.wv[word] for word in model.wv.index_to_key}\n",
    "\n",
    "print(word_embeddings)\n",
    "\n",
    "# Vectorise text using TF-IDF\n",
    "vectoriser = TfidfVectorizer(stop_words=\"english\", max_features=50)  # Limit features for efficiency\n",
    "\n",
    "X_tfidf = vectoriser.fit_transform(X)\n",
    "\n",
    "feature_names = vectoriser.get_feature_names_out()\n",
    "\n",
    "# Transpose TF-IDF matrix to get word embeddings (words as features)\n",
    "X_tfidf_transposed = X_tfidf.T.toarray()  # Each row corresponds to a word\n",
    "\n",
    "seed = 7\n",
    "\n",
    "# Reduce dimensionality with PCA before applying t-SNE\n",
    "pca = PCA(n_components=5, random_state=seed)  # Reduce to 5 dimensions max\n",
    "X_pca = pca.fit_transform(X_tfidf_transposed)\n",
    "\n",
    "# Apply t-SNE on PCA-reduced word embeddings\n",
    "tsne = TSNE(n_components=2, random_state=seed, perplexity=2)  # Lower perplexity for small datasets, we can increase this if using all the data\n",
    "X_embedded = tsne.fit_transform(X_pca)\n",
    "\n",
    "# Create scatter plot for word embeddings\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(X_embedded[:, 0], X_embedded[:, 1], alpha=0.7, color=\"purple\")\n",
    "\n",
    "# Annotate each point with its corresponding word\n",
    "for i, word in enumerate(feature_names):\n",
    "    plt.annotate(word, (X_embedded[i, 0], X_embedded[i, 1]), fontsize=9, alpha=0.75)\n",
    "\n",
    "plt.xlabel(\"t-SNE Component 1\")\n",
    "plt.ylabel(\"t-SNE Component 2\")\n",
    "\n",
    "plt.title(\"t-SNE visualisation of Word Embeddings\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IDsatUMFET8G",
   "metadata": {
    "id": "IDsatUMFET8G"
   },
   "source": [
    "From the plot we can see similar words cluster together.  This helps find semantic relationships in text data (e.g., words like king and queen appearing close). t-SNE preserves local relationships better than PCA.\n",
    "\n",
    "PCA keeps global structure but doesn't capture local similarities well.\n",
    "t-SNE maintains local similarities, ensuring words that were close in high dimensions stay close in 2D space. This makes it useful for understanding word embeddings and text data.\n",
    "\n",
    "If you use TF-IDF, and Word2Vec, t-SNE helps visualise how words are grouped. Therefore, we can use it to analyse sentiment-related words, topic clusters, or semantic relationships. In sentiment analysis, you might find clusters of positive and negative words, for instance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rn28qwEG7jhD",
   "metadata": {
    "id": "rn28qwEG7jhD"
   },
   "source": [
    "## What have we learnt?\n",
    "\n",
    "Through this exploration of feature selection techniques for language data, we have seen how different methods help improve text classification models by reducing noise, enhancing interpretability, and improving computational efficiency.\n",
    "\n",
    "We discussed manual and domain-specific methods, such as stopword removal, part-of-speech filtering, and named entity recognition to help refine feature selection by focusing on meaningful words.\n",
    "\n",
    "We covered statistical approaches like TF-IDF to highlight important but uncommon words, as well as, chi-square measures for word-category association. We looked at mutual information, which helps determine words that provide the most predictive value. We also saw that dimensionality reduction methods like Latent Semantic Analysis (LSA) and Non-Negative Matrix Factorisation (NMF) reduce the feature space while preserving key relationships, making models more efficient and interpretable.\n",
    "\n",
    "Embedded methods like Lasso regression and Random Forest-based selection are useful as they automatically identify important words while eliminating less useful ones. And neural network-based approaches like word embeddings capture semantic meaning beyond frequency counts.\n",
    "\n",
    "We demonstrated how to implement t-SNE visualisation to help in exploring high-dimensional text data and understanding how words cluster based on their relationships.\n",
    "\n",
    "In summary, we can enhance the accuracy and efficiency of our text classification models by applying these methods, ensuring that they focus on the most meaningful features. The choice of method depends on the dataset, model complexity, and computational resources available. Combining multiple techniques often leads to the best results in real-world applications."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
