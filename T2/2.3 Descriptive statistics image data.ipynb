{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HywjAqMia5mM"
   },
   "source": [
    "## Analysing, preprocessing, and transforming image datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3wnSYlC9a5mR"
   },
   "source": [
    "### Introduction\n",
    "\n",
    "We’ll explore how to load, inspect, and prepare an image dataset for use in machine learning and deep learning tasks. Working with image data involves a few extra steps compared to text or tabular data, but the principles are similar: understand what you have, clean and prepare it, and then structure it for modelling.\n",
    "\n",
    "We’ll use the popular *Cats and Dogs* dataset as an example—a classic binary classification task where the goal is to teach a model to tell the difference between images of cats and dogs.\n",
    "\n",
    "This walkthrough will cover five key areas:\n",
    "\n",
    "- *Loading the Image Dataset* – How to read images from folders, organise them with labels, and view a few examples to check that everything looks correct.\n",
    "\n",
    "- *Exploratory Data Analysis (EDA)* – Analysing basic properties of the dataset, such as how many images are in each class, and image size and number of channels.\n",
    "\n",
    "When you understand how to handle image data effectively, you can create more robust models and avoid common pitfalls like overfitting, poor generalisation, or input mismatches. This foundation is essential whether you're building a simple classifier or a more complex computer vision system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VMK7xkAGa5mS"
   },
   "source": [
    "### Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5147,
     "status": "ok",
     "timestamp": 1741817844204,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "is1iiItxa5mT",
    "outputId": "be1571a3-d4f8-4761-9d91-93d5de5b5d38"
   },
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib seaborn opencv-python tensorflow keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBR2ObNza5mU"
   },
   "source": [
    "### Downloading the dataset\n",
    "\n",
    "We will use the *Cats and Dogs dataset*, a well-known benchmark for image classification tasks. The dataset consists of images of two categories stored in a directory `./cats_dogs/train/`:\n",
    "\n",
    "```\n",
    "./cats_dogs/train/\n",
    "    cats/\n",
    "      cat_1.jpg\n",
    "      cat_2.jpg\n",
    "      ...\n",
    "    dogs/\n",
    "      dog_1.jpg\n",
    "      dog_2.jpg\n",
    "      ...\n",
    "```\n",
    "Once downloaded, we unpack the dataset and find that each image is stored in the relevant folder, which represents our target label, `cats/` or `dogs/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6446,
     "status": "ok",
     "timestamp": 1741817850650,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "xTQXLHLBbRzh",
    "outputId": "8d5df1dc-7ca6-4e70-979b-af6ca3b0532c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Define URL and target filenames\n",
    "url = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
    "zip_path = \"cats_and_dogs_filtered.zip\"\n",
    "extract_dir = \"cats_dogs\"\n",
    "\n",
    "# Download the zip file\n",
    "print(\"Downloading dataset...\")\n",
    "urllib.request.urlretrieve(url, zip_path)\n",
    "print(\"Download complete.\")\n",
    "\n",
    "# Extract the zip file\n",
    "print(\"Extracting files...\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "print(\"Extraction complete.\")\n",
    "\n",
    "# Delete the zip file\n",
    "print(\"Cleaning up...\")\n",
    "os.remove(zip_path)\n",
    "print(\"Cleanup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUoLloLObOmK"
   },
   "source": [
    "### Loading the image data\n",
    "\n",
    "To load the image data, we walk through each directory and collate the filenames. We first define the root directory for our data `dataset_path`, then the two classes in `category_path`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1741818155816,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "0L5tVYV4a5mV",
    "outputId": "71cd836d-9d47-431b-c32a-dc923b611267"
   },
   "outputs": [],
   "source": [
    "import os                    # For file and directory operations\n",
    "import cv2                   # OpenCV for image loading and processing\n",
    "import numpy as np           # For numerical operations (not used yet, but useful later)\n",
    "import matplotlib.pyplot as plt  # For plotting and visualization\n",
    "\n",
    "# Define the root path to the training dataset\n",
    "dataset_path = \"cats_dogs/cats_and_dogs_filtered/train/\"  # Adjust this to your dataset location\n",
    "\n",
    "# Define class labels - these should correspond to subfolders inside the dataset path\n",
    "category_path = [\"cats\", \"dogs\"]\n",
    "\n",
    "# Prepare lists to store image paths and their corresponding labels\n",
    "image_files = []\n",
    "labels = []\n",
    "\n",
    "limit = 500  # Maximum number of images to load per category (to reduce load for initial EDA)\n",
    "\n",
    "# Load up to `limit` images PER CATEGORY (cats and dogs)\n",
    "for category in category_path:\n",
    "    full_category_path = os.path.join(dataset_path, category)  # Full path to category folder\n",
    "    count = 0  # Counter for how many images we've added from this category\n",
    "\n",
    "    for file in os.listdir(full_category_path):\n",
    "        if file.endswith(('png', 'jpg', 'jpeg')):\n",
    "            image_files.append(os.path.join(full_category_path, file))\n",
    "            labels.append(category)\n",
    "            count += 1\n",
    "\n",
    "            if count >= limit:\n",
    "                break  # Stop after `limit` images from this category\n",
    "\n",
    "\n",
    "# Raise an error if no images were found (to catch path issues early)\n",
    "if not image_files:\n",
    "    raise ValueError(\"No images found! Check the dataset path and file extensions.\")\n",
    "\n",
    "# Output the number of images loaded\n",
    "print(f\"Total images loaded: {len(image_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLtc9unda5mV"
   },
   "source": [
    "### Visualising the dataset\n",
    "To get a quick visual overview of our image dataset, we randomly select a few pictures of cats and dogs and display them side by side. To achieve this, the code below performs the following:\n",
    "\n",
    "1. It picks 5 random cat images and 5 random dog images from the dataset.  \n",
    "2. It loads these images and converts them into a format suitable for displaying in colour.  \n",
    "3. It arranges the selected images in a tidy grid: one row for cats and another for dogs, each labelled with a simple title.\n",
    "\n",
    "This is a straightforward and helpful way to visually explore the data before training a machine learning model. It helps confirm that the images are loading correctly and gives us a sense of the kind of pictures the model will be learning from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "executionInfo": {
     "elapsed": 2068,
     "status": "ok",
     "timestamp": 1741818209599,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "_V6TFR0-a5mW",
    "outputId": "25c157cb-1cf5-41aa-b2a6-26065c644aea"
   },
   "outputs": [],
   "source": [
    "import random  # For selecting random samples\n",
    "\n",
    "# Select 5 random image paths for each category (cats and dogs)\n",
    "cat_images = random.sample([img for img, label in zip(image_files, labels) if label == 'cats'], 5)\n",
    "dog_images = random.sample([img for img, label in zip(image_files, labels) if label == 'dogs'], 5)\n",
    "\n",
    "# Load each selected image and convert it from BGR (OpenCV default) to RGB (for accurate colour display)\n",
    "cat_imgs = [cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB) for img in cat_images]\n",
    "dog_imgs = [cv2.cvtColor(cv2.imread(img), cv2.COLOR_BGR2RGB) for img in dog_images]\n",
    "\n",
    "# Create a 2-row grid of subplots: one row for cats, one for dogs, 5 images in each\n",
    "fig, ax = plt.subplots(2, 5, figsize=(10, 4))  # 2 rows × 5 columns of image slots\n",
    "\n",
    "# Loop to display the cat and dog images in the grid\n",
    "for i in range(5):\n",
    "    # Display each cat image in the top row\n",
    "    ax[0, i].imshow(cat_imgs[i])      # Show image\n",
    "    ax[0, i].axis('off')              # Hide axes for a cleaner look\n",
    "    ax[0, i].set_title(f'Cat {i+1}')  # Add title\n",
    "\n",
    "    # Display each dog image in the bottom row\n",
    "    ax[1, i].imshow(dog_imgs[i])\n",
    "    ax[1, i].axis('off')\n",
    "    ax[1, i].set_title(f'Dog {i+1}')\n",
    "\n",
    "# Add an overall title to the figure\n",
    "plt.suptitle(\"Random Samples of Cats and Dogs\", fontsize=14)\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rBDzW9Mwa5mX"
   },
   "source": [
    "### Descriptive statistics for images\n",
    "Exploratory Data Analysis (EDA) is a crucial step in machine learning and deep learning projects involving images. It helps us understand the dataset, detect potential imbalances, inconsistencies, or anomalies, and make informed preprocessing decisions before training models.\n",
    "\n",
    "For image datasets, EDA typically involves:\n",
    "\n",
    "- Checking dataset distribution (number of images per class).\n",
    "- Examining image sizes, resolutions, and color channels to standardise input data.\n",
    "- Visualising sample images to understand quality and variability.\n",
    "\n",
    "Before we train a machine learning model with our cat and dog images, it's important to understand what those images actually look like behind the scenes. We will check some basic properties of the images—specifically their *size* and *colour format*. \n",
    "\n",
    "Why does this matter?\n",
    "\n",
    "- *Image size matters*: If your images are all different sizes, they usually need to be resized to a consistent shape before training a model.\n",
    "- *Checking colour format*: Most models expect colour images with 3 channels (red, green, blue). If some images are black and white (1 channel), this could cause problems.\n",
    "- *Quality control*: Helps identify if any images are broken, too small, or in the wrong format before we go further.\n",
    "\n",
    "In short, this is a quick “health check” of the dataset to make sure the images are ready for machine learning.\n",
    "\n",
    "In this next step, we check the dimensions of each image: how tall and wide they are, and whether they're in colour. This gives us a quick overview of the dataset and helps us spot any problems—like images that are too small or in the wrong format:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quality control thresholds\n",
    "min_height = 100  # Minimum acceptable image height in pixels\n",
    "min_width = 100   # Minimum acceptable image width in pixels\n",
    "expected_channels = 3  # Expecting RGB images (3 colour channels)\n",
    "\n",
    "# Lists to collect any problematic images for review\n",
    "broken_images = []         # Images that couldn't be read (possibly corrupted)\n",
    "small_images = []          # Images that are smaller than the minimum size\n",
    "wrong_format_images = []   # Images with the wrong number of colour channels\n",
    "\n",
    "# Go through each image in the dataset\n",
    "for img_path in image_files:\n",
    "    img = cv2.imread(img_path)  # Attempt to read the image using OpenCV\n",
    "\n",
    "    if img is None:\n",
    "        # If image loading fails, mark it as broken\n",
    "        broken_images.append(img_path)\n",
    "        continue  # Skip to the next image\n",
    "\n",
    "    height, width, channels = img.shape  # Get image dimensions and number of channels\n",
    "\n",
    "    # Check if image is too small\n",
    "    if height < min_height or width < min_width:\n",
    "        small_images.append((img_path, (width, height)))  # Save path and actual size\n",
    "\n",
    "    # Check if image does not have the expected number of colour channels (RGB = 3)\n",
    "    if channels != expected_channels:\n",
    "        wrong_format_images.append((img_path, channels))  # Save path and number of channels\n",
    "\n",
    "# Print summary of quality check results\n",
    "print(f\"Broken images (unreadable): {len(broken_images)}\")\n",
    "print(f\"Images too small (< {min_width}x{min_height}): {len(small_images)}\")\n",
    "print(f\"Images with unexpected number of channels: {len(wrong_format_images)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally print examples from each issue category (up to 5)\n",
    "if broken_images:\n",
    "    print(\"\\nExample broken images:\")\n",
    "    print(\"\\n\".join(broken_images[:5]))\n",
    "\n",
    "if small_images:\n",
    "    print(\"\\nExample small images:\")\n",
    "    for path, size in small_images[:5]:\n",
    "        print(f\"{path} - {size}\")\n",
    "\n",
    "if wrong_format_images:\n",
    "    print(\"\\nExample wrong format images:\")\n",
    "    for path, ch in wrong_format_images[:5]:\n",
    "        print(f\"{path} - {ch} channels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, calculating the average image size and checking how many colour channels are used, ensures the images are consistent and ready to be processed. It’s a simple but crucial step to avoid issues later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1220,
     "status": "ok",
     "timestamp": 1741818219091,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "7ENPfNqUa5mX",
    "outputId": "1111f2e5-6d7e-4434-b174-1437b91b38d2"
   },
   "outputs": [],
   "source": [
    "# Check image shapes\n",
    "image_shapes = [cv2.imread(img).shape for img in image_files]\n",
    "heights, widths, channels = zip(*image_shapes)\n",
    "\n",
    "print(f'Number of images: {len(image_files)}')\n",
    "print(f'Average Image Height: {np.mean(heights):.2f}')\n",
    "print(f'Average Image Width: {np.mean(widths):.2f}')\n",
    "print(f'Number of Channels: {set(channels)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukzzareDa5mY"
   },
   "source": [
    "One important aspect of data analysis is checking whether our dataset is balanced. Below, we generate a *bar chart* to visualise how many images are present per class. This helps us understand whether we need to apply techniques such as *data augmentation* to expand one or more of the classes, to improve model training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4898,
     "status": "ok",
     "timestamp": 1741818267139,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "XmBmRnk1a5mY",
    "outputId": "e79df9e4-16fa-4f38-adce-9343fb190848"
   },
   "outputs": [],
   "source": [
    "import os              # For working with file and folder paths\n",
    "import numpy as np     # For handling image data as arrays\n",
    "from PIL import Image  # Pillow library for opening and processing images\n",
    "\n",
    "# Define the two classes in the dataset, stored in separate folders\n",
    "classes = ['dogs', 'cats']\n",
    "\n",
    "# Lists to store the image data (X) and the corresponding labels (y)\n",
    "X = []  # Will contain image arrays\n",
    "Y = []  # Will contain class labels (0 for dogs, 1 for cats)\n",
    "\n",
    "# Create a dictionary to count how many images are loaded from each class\n",
    "class_counts = {class_name: 0 for class_name in classes}\n",
    "\n",
    "# Loop through each class folder and process the images\n",
    "for label_index, class_name in enumerate(classes):\n",
    "    class_path = os.path.join(dataset_path, class_name)  # Full path to the class folder\n",
    "\n",
    "    # Loop through all files in the folder\n",
    "    for filename in os.listdir(class_path):\n",
    "        if filename.endswith('.jpg'):  # Only process .jpg image files\n",
    "            img_path = os.path.join(class_path, filename)  # Full path to the image\n",
    "\n",
    "            # Open the image and ensure it's in RGB format (3 colour channels)\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "            # Resize the image to 64x64 pixels for consistency and lower computation\n",
    "            img = img.resize((64, 64))\n",
    "\n",
    "            # Convert the image to a NumPy array so it can be used in machine learning\n",
    "            img_array = np.array(img)  # Shape will be (64, 64, 3)\n",
    "            X.append(img_array)        # Add image data to the list\n",
    "            Y.append(label_index)      # Add numeric label (0 or 1)\n",
    "\n",
    "            # Update the count for this class\n",
    "            class_counts[class_name] += 1\n",
    "\n",
    "# Print how many images were loaded from each class\n",
    "print(class_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 427
    },
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1741818296686,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "GoHJfzwva5mZ",
    "outputId": "0b2900cd-7065-4246-fc65-5b377209781a"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color=['blue', 'orange'])\n",
    "\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Images')\n",
    "\n",
    "plt.title('Dataset Distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, we've taken a sample of the data with an equal number of images from each class. However, in real-world scenarios, datasets are often *not* perfectly balanced—unless they've been deliberately curated that way. That's why checking the class distribution is an important first step, as it helps you anticipate any issues you might need to address depending on the type of model you're planning to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sknkvci4a5mZ"
   },
   "source": [
    "### Visualising the impact of preprocessing\n",
    "\n",
    "Preprocessing is a crucial step in preparing image data for machine learning and deep learning models. It ensures consistency across the dataset, improves computational efficiency, and enhances model performance. The three key preprocessing techniques covered here are:\n",
    "\n",
    "- Resizing – Ensures all images have the same dimensions, as deep learning models require uniform input sizes.\n",
    "- Normalisation – Scales pixel values to a standard range (e.g., 0 to 1) to stabilise training and improve convergence.\n",
    "- Greyscale Conversion – Converts RGB images to a single channel to reduce complexity, especially when colour is not a significant feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49QNWpFYa5mZ"
   },
   "source": [
    "### Resizing Images\n",
    "Images in real-world datasets come in different sizes, as we have seen above. To ensure consistency, we resize them to a fixed shape before feeding them into a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WfSWpst1a5ma"
   },
   "source": [
    "### Normalising images\n",
    "Normalisation scales pixel values to a range (e.g., 0 to 1) instead of 0 to 255. This helps to improve model convergence during training, prevent numerical instability, and standardise input for neural networks. In short, it prevents large pixel values from dominating in deep learning models, and ensures consistency across different images.\n",
    "\n",
    "Below we write a function `preprocess_image`, to perform resizing, and normalisation in one go before visualising the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2381,
     "status": "ok",
     "timestamp": 1741818319916,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "flKjDJTIa5mZ",
    "outputId": "76adb39d-6f05-46e6-e528-dfca1b01bd76"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Define a function to resize and normalise an image\n",
    "def preprocess_image(image_path, target_size=(128, 128)):\n",
    "    image = cv2.imread(image_path)  # Read image\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    image = cv2.resize(image, target_size)  # Resize image\n",
    "    image = image / 255.0  # Normalise pixel values\n",
    "    return image\n",
    "\n",
    "# Ensure image_files is defined\n",
    "if 'image_files' not in locals():\n",
    "    raise ValueError(\"Error: image_files is not defined. Load the dataset first.\")\n",
    "\n",
    "# Apply preprocessing to all images\n",
    "processed_images = np.array([preprocess_image(img) for img in image_files])\n",
    "\n",
    "print(f'Processed Image Dataset Shape: {processed_images.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values returned as the image dataset shape `(1000, 128, 128, 3)`, describe the structure of our processed image data in numerical terms. Here’s what each number means:\n",
    "\n",
    "- 1000 – The total number of images in the dataset\n",
    "\n",
    "- 128 – The height of each image in pixels (after resizing)\n",
    "\n",
    "- 128 – The width of each image in pixels\n",
    "\n",
    "- 3 – The number of colour channels (Red, Green, Blue)\n",
    "\n",
    "So this tells us that we have 1,000 colour images, each resized to 128×128 pixels, ready for model training or further analysis. To visualise the effects of preprocessing, we will select a random image to ensure everything appears as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457
    },
    "executionInfo": {
     "elapsed": 874,
     "status": "ok",
     "timestamp": 1741818912117,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "ZQv8FqfDa5ma",
    "outputId": "fe44d545-ebc1-410b-d5b5-c9d28072c690"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Select a random sample image\n",
    "sample_image_path = random.choice(image_files)\n",
    "\n",
    "# Select 5 random images for visualisation\n",
    "sample_indices = random.sample(range(len(image_files)), 5)\n",
    "\n",
    "# Plot original and resized images side by side\n",
    "fig, ax = plt.subplots(2, 5, figsize=(10, 5))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Load original image\n",
    "    original = cv2.cvtColor(cv2.imread(image_files[idx]), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get resized version\n",
    "    resized = processed_images[idx]  # normalised and resized\n",
    "\n",
    "    # Show original\n",
    "    ax[0, i].imshow(original)\n",
    "    ax[0, i].axis('off')\n",
    "    ax[0, i].set_title(f\"Original ({original.shape[1]}x{original.shape[0]})\")\n",
    "\n",
    "    # Show resized\n",
    "    ax[1, i].imshow(resized)\n",
    "    ax[1, i].axis('off')\n",
    "    ax[1, i].set_title(\"Resized (128x128)\")\n",
    "\n",
    "plt.suptitle(\"Original vs. Resized Images\", fontsize=14)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBNt-QY2a5ma"
   },
   "source": [
    "### Converting images to Greyscale\n",
    "If colour is not an important feature, greyscale images reduce the number of channels from 3 (RGB) to 1, decreasing model complexity. Plotting the results, enables us to check how successful this step was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "executionInfo": {
     "elapsed": 2241,
     "status": "ok",
     "timestamp": 1741818595318,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "lTocoA6aa5ma",
    "outputId": "c78b2d32-54a4-4f71-8813-6010d630adfe"
   },
   "outputs": [],
   "source": [
    "# Define a function to convert an image to greyscale\n",
    "def convert_to_greyscale(image_path, target_size=(128, 128)):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)  # We read image in greyscale mode\n",
    "    image = cv2.resize(image, target_size)  # Resize image\n",
    "    image = image / 255.0  # Normalise pixel values\n",
    "    return image\n",
    "\n",
    "# Apply greyscale conversion to all images\n",
    "greyscale_images = np.array([convert_to_greyscale(img) for img in image_files])\n",
    "\n",
    "# Display a sample greyscale image\n",
    "plt.imshow(greyscale_images[0], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(\"Greyscale Image (128x128)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sZ8PuDJa5md"
   },
   "source": [
    "This reduces computational complexity (fewer channels = fewer computations). So, this is useful for tasks like digit recognition (MNIST) where colour is irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GqwDroiNa5md"
   },
   "source": [
    "### Showing a comparison: original vs. preprocessed\n",
    "Lets now visualise and compare the outputs of all preprocessing steps including the original, resized, normalised, and greyscale image variants across our sample of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "executionInfo": {
     "elapsed": 441,
     "status": "ok",
     "timestamp": 1741818599563,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "fnYkBKNza5md",
    "outputId": "5467977a-28f8-4171-9912-80f45842f76b"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Select 5 random images for visualisation\n",
    "sample_indices = random.sample(range(len(image_files)), 5)\n",
    "\n",
    "# Plot original, resized, and greyscale images side by side\n",
    "fig, ax = plt.subplots(3, 5, figsize=(15, 9))\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    # Load original image (BGR to RGB)\n",
    "    original = cv2.cvtColor(cv2.imread(image_files[idx]), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Get resized version (already normalised and 128x128)\n",
    "    resized = processed_images[idx]\n",
    "\n",
    "    # Convert resized image to greyscale\n",
    "    greyscale = cv2.cvtColor((resized * 255).astype(np.uint8), cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Show original (top row)\n",
    "    ax[0, i].imshow(original)\n",
    "    ax[0, i].axis('off')\n",
    "    ax[0, i].set_title(f\"Original ({original.shape[1]}x{original.shape[0]})\")\n",
    "\n",
    "    # Show resized colour image (middle row)\n",
    "    ax[1, i].imshow(resized)\n",
    "    ax[1, i].axis('off')\n",
    "    ax[1, i].set_title(\"Resized (128x128)\")\n",
    "\n",
    "    # Show greyscale version (bottom row)\n",
    "    ax[2, i].imshow(greyscale, cmap='gray')\n",
    "    ax[2, i].axis('off')\n",
    "    ax[2, i].set_title(\"Greyscale (128x128)\")\n",
    "\n",
    "plt.suptitle(\"Original, Resized, and Greyscale Versions\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What have we learnt?\n",
    "Exploring your data first is essential. It helps you spot problems early—like missing images, wrong formats, or uneven numbers of cats and dogs.  Looking at sample images lets you check that everything looks right before training a model.  Checking image sizes and formats makes sure they’re consistent, which is important for the model to work properly.  Simple stats and visual checks help you understand what kind of data you're working with and whether anything needs fixing.  \n",
    "\n",
    "The key takeaway is that good preparation leads to better results. Taking time to explore and clean your data makes your model more accurate and reliable in the long run."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
