{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_1FLp6JHs-M"
   },
   "source": [
    "## Resampling Techniques\n",
    "\n",
    "### Introduction\n",
    "Imagine you're baking a cake and want to be sure the recipe works every time—not just by luck. You'd probably test it repeatedly, perhaps slightly changing ingredients or oven temperatures, to check its reliability.\n",
    "\n",
    "Resampling techniques in machine learning work a bit like that. They help us understand how well our models might perform by repeatedly testing them with different subsets of the same data.  When we are \"shuffling\" or reusing the data we already have through resampling methods, we can gain confidence in our results, even if we don't have extra data.\n",
    "\n",
    "What we want to do, is test how well our model will perform if we received data that it hasn't seen. When we say unseen data, this may be new future data we receive when our model goes into production, or if we were to receive additional historic data.\n",
    "\n",
    "Essentially, resampling helps us to know whether our machine learning approach is truly reliable or just a series of lucky guesses.\n",
    "\n",
    "We will explore how resampling techniques can be applied effectively across three distinct types of data.\n",
    "First, we examine *numeric data* using the Pima Indians Diabetes dataset, illustrating how resampling helps improve predictions about diabetes risk.\n",
    "\n",
    "Next, we look at *language data*, specifically using sentiment analysis—a method of identifying emotions or opinions from written language—to show how resampling enhances understanding of text-based information.\n",
    "\n",
    "Finally, we apply these techniques to *image data* with the well-known Cats vs. Dogs dataset, highlighting how resampling assists machine learning models in accurately recognising visual patterns.\n",
    "\n",
    "This varied approach showcases the versatility and practicality of resampling across different types of real-world data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RMyRZtlbHs-P"
   },
   "source": [
    "### Types of resampling\n",
    "\n",
    "For each type of data we apply four widely-used resampling methods, these include:\n",
    "\n",
    "- *Train/Test Split*:\n",
    "We divide the dataset into two parts: one set for training the model, and the other set to test how well it predicts new, unseen data.\n",
    "\n",
    "- *k-Fold Cross-Validation*:\n",
    "Here, we split the data into $k$ smaller subsets (or folds). Each fold takes turns being the test set, while the remaining folds train the model. This way, every data point is tested exactly once, giving a balanced evaluation.\n",
    "\n",
    "- *Leave-One-Out Cross-Validation (LOOCV)*:\n",
    "This method is a special case of cross-validation, where each data point gets its own turn to be the test set. The model trains on all other points, ensuring thorough evaluation, especially useful with smaller datasets.\n",
    "\n",
    "- *Repeated Random Test-Train Splits (Shuffle Split)*:\n",
    "Instead of splitting data once, we randomly split it many times. Each split results in a slightly different train and test set. This repeated shuffling helps us understand how stable our model's performance is.\n",
    "\n",
    "We apply different resampling techniques to a structured numeric dataset, the Pima Indian's dataset on Diabetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DlByVUCNHs-Q"
   },
   "source": [
    "### Install Python libaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11245,
     "status": "ok",
     "timestamp": 1742402365012,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "h7HPFYQhHs-Q",
    "outputId": "a9e2d019-ec38-4587-b541-d02183951c96"
   },
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn opencv-python matplotlib nltk seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmxsYVx8Hs-R"
   },
   "source": [
    "### Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 1045,
     "status": "ok",
     "timestamp": 1742402366055,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "VkQCUhzVHs-R",
    "outputId": "20fc504a-8f3d-4d05-8a8e-1471c344b08a"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://raw.githubusercontent.com/martyn-harris-bbk/AppliedMachineLearning/main/data/pima-indians-diabetes.data.csv'\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "print(\"Download complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "filename = 'pima-indians-diabetes.data.csv'\n",
    "\n",
    "header = [\n",
    "    'Pregnancy_Count',\n",
    "    'Glucone_conc',\n",
    "    'Blood_pressure',\n",
    "    'Skin_thickness',\n",
    "    'Insulin',\n",
    "    'BMI',\n",
    "    'DPF',\n",
    "    'Age',\n",
    "    'Class'\n",
    "]\n",
    "\n",
    "data = pd.read_csv(filename, names=header)\n",
    "\n",
    "print(data.shape)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RUFMCdTsHs-S"
   },
   "source": [
    "## Machine Learning model\n",
    "To evaluate the effectiveness of various resampling techniques, we use a Logistic Regression model.\n",
    "\n",
    "Logistic regression is particularly well-suited for classification problems where the target variable has two categories, such as determining if someone has diabetes or not, classifying sentiment as positive or negative, or distinguishing between images of cats and dogs.\n",
    "\n",
    "We specifically choose the `'liblinear'` solver because it's efficient, stable, and performs well with smaller to medium-sized datasets, ensuring quick and reliable results during our comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2402,
     "status": "ok",
     "timestamp": 1742402368455,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "81Yydw8dHs-S"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WB41koCOHs-S"
   },
   "source": [
    "We start by clearly identifying two components in our dataset: the **training data (X)** and the **target variable (Y)**. The training data (X) includes the input features or information we use to make predictions, while the target variable (Y) is the outcome or label we want our model to predict. Clearly separating X and Y is essential to train and evaluate models effectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1742402368482,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "dNaXp7W6Hs-S"
   },
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1].values\n",
    "Y = data.iloc[:, -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1x7DC3zHs-T"
   },
   "source": [
    "### Train/Test Split  \n",
    "Train/test split is one of the simplest and most commonly used techniques in machine learning. The idea is to divide the full dataset into two parts:\n",
    "- The *training set* is used to teach the model by showing it patterns and relationships in the data.\n",
    "- The *test set* is used to evaluate how well the model performs on new, unseen data — simulating how it might work in the real world.\n",
    "\n",
    "#### Advantages:\n",
    "- *Simplicity and speed*: It is very easy to use and quick to run, making it ideal for early testing.  \n",
    "- *Clear evaluation*: It gives a straightforward way to check how the model performs on independent data.  \n",
    "- *Low computational cost*: It only requires one round of training and testing, which keeps things efficient.\n",
    "\n",
    "#### Disadvantages:\n",
    "- *Risk of bias*: The model’s performance can depend heavily on how the data was split — one lucky or unlucky test set can affect the result.  \n",
    "- *Less reliable with small datasets*: If you have limited data, this method might not give an accurate picture of performance.  \n",
    "- *Limited use of data*: Only part of the data is used for training, so potentially useful information might be left out.\n",
    "\n",
    "Overall, the train/test split approach is useful for quick experiments and early insights when building a model. However, it should not be the only evaluation method you rely on, especially if your dataset is small or if high accuracy is critical.\n",
    "\n",
    "In the example below, we divide the dataset `(X, Y)` into two parts:\n",
    "- *67%* for training the model (`X_train`, `Y_train`)  \n",
    "- *33%* for testing the model's accuracy (`X_test`, `Y_test`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42,
     "status": "ok",
     "timestamp": 1742402368525,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "vwe8vHwuHs-T",
    "outputId": "5e6d3a2a-6df7-4326-86f5-f4f44c0256a9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # For splitting data into training and test sets\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression classifier\n",
    "\n",
    "# Create a logistic regression model\n",
    "# 'liblinear' solver is efficient for small datasets and supports L1 regularisation\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Set a random seed for reproducibility\n",
    "seed = 7\n",
    "\n",
    "# Split the dataset into training (67%) and test (33%) sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=seed)\n",
    "\n",
    "# Train (fit) the logistic regression model using the training data\n",
    "model.fit(X_train, Y_train)\n",
    "\n",
    "# Evaluate the model's accuracy on the test set and print it as a percentage with 3 decimal places\n",
    "print(f'Accuracy: {model.score(X_test, Y_test) * 100:.3f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uqm_KEVrHs-T"
   },
   "source": [
    "### K-Fold Cross-Validation  \n",
    "This method splits our dataset into smaller groups called *“folds”*, randomly shuffling the data first to ensure fairness and variety in each group.\n",
    "\n",
    "#### Advantages:\n",
    "- *Reduced bias*: Every data point is used for both training and testing, which gives a more balanced and fair assessment of model performance.  \n",
    "- *Efficient use of data*: The entire dataset is fully used, so even with smaller datasets, we get the most out of the available data.  \n",
    "- *Stability and consistency*: Since the model is tested multiple times, we can average the results to get a more reliable and consistent estimate of how well it performs.\n",
    "\n",
    "#### Disadvantages:\n",
    "- *Increased computational cost*: Because the model is trained and tested multiple times (once for each fold), the process takes longer than a single train/test split.  \n",
    "- *Risk of variance with small datasets*: If the dataset is very small or unevenly balanced, some test folds might give misleading results.  \n",
    "- *Sensitive to choice of \"k\"*: Picking the right number of folds is important—too few can lead to inaccurate results, while too many can make the process unnecessarily slow.\n",
    "\n",
    "Overall, k-Fold Cross-Validation usually provides a more reliable and thorough assessment of a model’s performance than a single train/test split. It is especially useful when working with limited data.\n",
    "\n",
    "We apply k-Fold Cross-Validation using `KFold(n_splits=10, shuffle=True)`. This means the model is trained and tested ten times, each time using a different fold as the test set and the remaining nine as the training set.\n",
    "\n",
    "Finally, we calculate the average accuracy and look at how much the results vary across the 10 runs. This gives us a clearer picture of how well the model is likely to perform on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 345,
     "status": "ok",
     "timestamp": 1742402368869,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "aFjmlO7uHs-T",
    "outputId": "b4850c87-c156-421d-b02e-e6e22308b4fe"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# k-Fold Cross-Validation\n",
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "\n",
    "results = cross_val_score(model, X, Y, cv=kfold)\n",
    "\n",
    "print(f'k-Fold Accuracy: {results.mean()*100:.3f}% ({results.std()*100:.3f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIdt6cNjHs-T"
   },
   "source": [
    "<div style=\"border: 2px solid silver; border-radius: 5px; background-color: transparent;padding:10px;width:95%;margin: 10px;\">\n",
    "  <strong> Standard Deviation in Cross-Validation</strong>  \n",
    "<br><br>  \n",
    "The standard deviation in cross-validation helps us understand how much the model's accuracy varies from one fold to another. It is a measure of <em>consistency</em> in performance.\n",
    "\n",
    "What it tells us:\n",
    "- <em>Low standard deviation</em>: <br>\n",
    "The model performs similarly across all folds. This suggests stable, reliable predictions regardless of which part of the data is used — a sign of a trustworthy model.<br><br>  \n",
    "- <em>High standard deviation</em>:<br> \n",
    "The model's performance changes a lot across folds. It may do well on some subsets but poorly on others. This suggests the model might be sensitive to specific data or possibly overfitting.\n",
    "\n",
    "In short, standard deviation gives us an idea of how dependable the model’s reported accuracy really is. A small value is a good sign; a large value means we should investigate further.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT6dETXy5Jvh"
   },
   "source": [
    "### Leave-One-Out Cross-Validation (LOO-CV)\n",
    "Leave-One-Out Cross-Validation (LOO-CV) is a technique used to evaluate the performance of a machine learning model by training and testing it multiple times on slightly different subsets of data. In this method, for a dataset with *n* data points, the model is trained on *n-1* points while leaving out one data point for testing.\n",
    "\n",
    "This process repeats *n* times, with each data point getting a turn as the test set exactly once. The final model performance is typically averaged over all iterations to get an estimate of how well the model generalises to new data.\n",
    "\n",
    "#### Advantages:\n",
    "- *Maximises training data utilisation*: Since LOO-CV uses *n-1* samples for training and only *1* for testing, it ensures that the model is trained on nearly all available data, which can be beneficial for small datasets.\n",
    "\n",
    "- *Unbiased estimate of generalisation error*: As each sample is tested independently, LOO-CV provides a nearly unbiased estimate of how the model will perform on unseen data.\n",
    "\n",
    "- *No need for Train-Test Split decisions*: Unlike standard train-test splits, LOO-CV systematically evaluates the model on every data point, reducing the variance introduced by arbitrary split choices like those obtain through the regular test-train split method.\n",
    "\n",
    "- *Good for small datasets*: When your dataset is very limited, removing one data point at a time ensures that the model still gets trained on almost all available data. In short, LOO-CV is valuable in scenarios where every data point is crucial, such as medical predictions or rare-event modeling.\n",
    "\n",
    "#### Disadvantages:\n",
    "- *Extremely computationally expensive*: LOO-CV requires *n* separate model training runs, where *n* is the total number of samples. If the dataset is large (e.g., thousands of images), this can be highly inefficient and time-consuming, especially for complex models like deep learning networks.\n",
    "\n",
    "- *High variance in evaluation scores*: As each test set consists of just one sample, the error estimate has high variance. If a sample is difficult to classify, it may significantly impact the overall performance estimate.\n",
    "\n",
    "- *Not ideal for model selection*: Unlike K-Fold Cross-Validation, which smooths out fluctuations, LOO-CV is too sensitive to individual samples. This makes it less reliable for selecting hyperparameters, as small variations can lead to misleading conclusions.\n",
    "\n",
    "- *May not reflect real-world generalisation*: In practice, models are tested on larger test sets, whereas LOO-CV only tests on a single instance at a time.\n",
    "This does not always mimic real-world performance, where the test set is typically larger and provides a more stable estimate.\n",
    "\n",
    "- *Poor performance with noisy data*: If the dataset contains noise (e.g., mislabeled images), LOO-CV may overemphasise these outliers, leading to unreliable performance metrics. A single misclassified noisy sample can significantly skew the results.\n",
    "\n",
    "- *Risk of overfitting*: Since the model is trained on almost the entire dataset, there is a higher risk that it will become too specialised to the training data.\n",
    "This can lead to overfitting, where the model performs well during validation but fails to generalise to completely unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15346,
     "status": "ok",
     "timestamp": 1742402384220,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "c6_tOByH5ZSi",
    "outputId": "c6d62ef0-42ef-49a7-d910-0739bbb2e0d9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut  # Leave-One-Out Cross-Validation tool\n",
    "from sklearn.metrics import accuracy_score  # To measure model accuracy\n",
    "from sklearn.linear_model import LogisticRegression  # Logistic regression classifier\n",
    "\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Set up Leave-One-Out Cross-Validation\n",
    "# This technique uses one data point as the test set and the rest for training,\n",
    "# repeating this process once for every point in the dataset\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Lists to store actual labels and predicted labels from each fold\n",
    "y_true = []\n",
    "y_predicted = []\n",
    "\n",
    "# Perform Leave-One-Out Cross-Validation\n",
    "for train_index, test_index in loo.split(X):\n",
    "    # Split the data into training and test sets for this fold\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Train the logistic regression model on the training data\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict the label for the one test instance\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Store the true label and the predicted label\n",
    "    y_true.append(y_test[0])\n",
    "    y_predicted.append(y_pred[0])\n",
    "\n",
    "# Calculate and display overall accuracy\n",
    "accuracy = accuracy_score(y_true, y_predicted)\n",
    "print(f'LOO-CV Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynW46eKI5fT5"
   },
   "source": [
    "### Repeated random Test-Train Splits\n",
    "Repeated Random Test-Train Splits, also known as Monte Carlo Cross-Validation, is a method used to assess the performance of a machine learning model. Unlike k-Fold Cross-Validation, where the dataset is split into a fixed number of folds, this method involves randomly dividing the dataset into training and test sets multiple times. Each split is independent, meaning a data point can appear in both training and test sets across different iterations. The model is trained and tested on different subsets each time, and the overall performance is averaged across all runs.\n",
    "\n",
    "#### Advantages:\n",
    "- Reduces variance compared to a single Train-Test Split: As we are repeating the process multiple times with different random splits, this method provides a more stable estimate of model performance. A single train-test split can be highly dependent on the specific split, whereas repetition reduces this issue.\n",
    "\n",
    "- More computationally efficient than K-Fold or LOO-CV: Unlike LOO-CV (which requires *n* model runs) and K-Fold Cross Validation (which runs *K* times), repeated random splitting allows you to control the number of repetitions, making it computationally more efficient.\n",
    "\n",
    "- More flexible than K-Fold Cross-Validation: You can control both the train-test ratio and the number of repetitions, giving you flexibility to adjust based on dataset size and computational power.\n",
    "\n",
    "- Works well for large datasets: Since each split is independent, it can be used on large datasets where K-Fold CV or LOO-CV may be too slow.\n",
    "\n",
    "- Helps prevent overfitting to a specific split: The model avoids overfitting to a specific train-test division by evaluating multiple different splits, leading to a more generalisable model.\n",
    "\n",
    "#### Disadvantages:\n",
    "- Some data points may not be included in the test set: Since splits are random, some samples may never appear in the test set, while others may be selected multiple times. This can introduce bias, as the model might not be tested on all examples equally.\n",
    "\n",
    "- Less efficient use of data compared to K-Fold Cross Validation: In K-Fold Cross-Validation, every sample appears in both train and test sets at least once. In Repeated Random Splits, some data points may never be tested, making it less efficient in using all available data.\n",
    "\n",
    "- Not always suitable for small datasets: If the dataset is small, random splits can lead to significant variations in results. A bad split where an entire class is underrepresented can distort model performance estimates.\n",
    "\n",
    "- Higher risk of data leakage: If preprocessing steps (such as normalisation or feature scaling) are applied before splitting, information from the test set might unintentionally influence training. Ensuring that each split is properly handled can be more error-prone compared to K-Fold CV.\n",
    "\n",
    "- Computational cost can still be high: The commputational cost is cheaper than LOO-CV, but running multiple repeated splits still increases the training time compared to a single train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1742402384420,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "-WulJpig6M6Q",
    "outputId": "aa473c41-f5de-4b2a-ada7-3af085060b89"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Define Repeated Random Test-Train Splits\n",
    "num_splits = 10  # Number of repetitions\n",
    "test_size = 0.3  # 30% of data is used for testing\n",
    "\n",
    "shuffle_split = ShuffleSplit(n_splits=num_splits, test_size=test_size, random_state=7)\n",
    "\n",
    "# Evaluate model using repeated random test-train splits\n",
    "scores = cross_val_score(model, X, Y, cv=shuffle_split)\n",
    "\n",
    "# Print the results\n",
    "print(f'Mean Accuracy: {np.mean(scores):.3f}, Standard Deviation: {np.std(scores):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGuD3OzaHs-T"
   },
   "source": [
    "## Resampling for Language data\n",
    "\n",
    "When applying resampling techniques to text data, we first transform the text into numerical form—typically using *TF-IDF vectorisation*. TF-IDF converts textual data into numerical feature vectors based on word frequency and importance. This step makes text data compatible with machine learning algorithms like logistic regression.\n",
    "\n",
    "Although this preprocessing step differs from numerical datasets (such as the Pima Diabetes dataset, which is already numeric), the core resampling approach remains similar:\n",
    "\n",
    "- We still perform methods such as *train/test splits*, *k-fold cross-validation*, *Leave-One-Out Cross-Validation*, and *Shuffle splits*.\n",
    "- The key difference is the additional step of converting text into numeric features before applying these resampling methods.\n",
    "\n",
    "This ensures that text classification models are robustly evaluated in the same systematic way as models trained on purely numeric data.\n",
    "\n",
    "The code below demonstrates how text data can be loaded and prepared for sentiment classification. It begins by importing the necessary libraries and defining the data's location, with categories labelled as positive (`pos`) and negative (`neg`).\n",
    "\n",
    "Text files from each category are read one by one, with the content appended to a list (`X_text`) and the corresponding labels (indicating positive or negative sentiment) appended to another list (`y_text`):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdL59ZGgRzBg"
   },
   "source": [
    "### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 30975,
     "status": "ok",
     "timestamp": 1742402415398,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "mABD4XGAQ3-i"
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "import os\n",
    "\n",
    "# IMDb dataset URL\n",
    "# url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\" # Size 80.2MB\n",
    "url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/mix20_rand700_tokens_0211.tar.gz\" # Size 2.2MB\n",
    "\n",
    "# Download the dataset to the current directory\n",
    "urllib.request.urlretrieve(url, \"aclImdb_v1.tar.gz\") \n",
    "\n",
    "# Unpack (extract) the dataset\n",
    "with tarfile.open(\"aclImdb_v1.tar.gz\", \"r:gz\") as tar:\n",
    "    tar.extractall()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAF_hCKZiJNE"
   },
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 51,
     "status": "ok",
     "timestamp": 1742402415466,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "FDpmaMwjHs-T",
    "outputId": "321c79a8-9fb4-4353-fbbc-fea10d4aae5e"
   },
   "outputs": [],
   "source": [
    "import os  # For accessing file system\n",
    "import numpy as np  # Numerical operations\n",
    "import random  # Random sampling of files\n",
    "\n",
    "# Directory containing text data organised into 'pos' and 'neg' folders\n",
    "root_dir = 'tokens/'\n",
    "categories = ['pos', 'neg']  # Sentiment categories: positive and negative\n",
    "\n",
    "X, Y = [], []  # Lists to store data (reviews) and labels (sentiments)\n",
    "\n",
    "sample_size = 250  # Number of samples per category (limited for quicker demostration)\n",
    "\n",
    "# Loop through each sentiment category\n",
    "for cat in categories:\n",
    "    file_list = os.listdir(os.path.join(root_dir, cat))  # List files in category folder\n",
    "    sample_files = random.sample(file_list, sample_size)  # Randomly pick files for sampling\n",
    "\n",
    "    # Read each sampled file\n",
    "    for filename in sample_files:\n",
    "        with open(os.path.join(root_dir, cat, filename), 'r') as f:\n",
    "            label = 1 if cat == \"pos\" else 0  # Assign numeric label (positive=1, negative=0)\n",
    "            \n",
    "            X.append(f.read())  # Append review text\n",
    "            Y.append(label)     # Append sentiment label\n",
    "\n",
    "Y = np.array(Y)  # Convert labels list to NumPy array for easier computation later\n",
    "\n",
    "# Display size of loaded dataset and a sample review with its sentiment label\n",
    "print(\"Training size:\", len(X), \"Labels:\", len(Y))\n",
    "print(\"Sample Review:\", X[0])\n",
    "print(\"Sentiment (0-neg, 1-pos):\", Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKEooLxFHs-U"
   },
   "source": [
    "### Preprocessing\n",
    "The data is converted into numerical form using TF-IDF vectorisation. The `TfidfVectorizer` is configured to remove common English words (`stop_words='english'`) and limit the features to the 3,000 most significant terms (`max_features=3000`).\n",
    "\n",
    "The vectoriser then processes the raw text data (`X_text`) and transforms it into a numerical format (`X_tfidf`) suitable for training our machine learning model. This process ensures the models focus on meaningful words and reduces the computational complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1742402415674,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "3zShT8EgHs-U",
    "outputId": "988cfd34-7cb1-446f-bbe9-c47f72b5c9ed"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectoriser = TfidfVectorizer(stop_words='english', max_features=3000)\n",
    "\n",
    "X_tfidf = vectoriser.fit_transform(X)\n",
    "\n",
    "print(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pGxZ-YhRHs-U"
   },
   "source": [
    "### Train/Test split\n",
    "We use the train/test split approach again, applied to our TF-IDF vectorised text data. The data (`X_tfidf`) is divided into a training set (80%) and a test set (20%), ensuring reproducibility with `random_state=7`.\n",
    "\n",
    "Our logistic regression model is then trained using the training data (`X_train`, `y_train`).\n",
    "\n",
    "Finally, we evaluate and report the accuracy of the model on unseen text data (`X_test`, `y_test`), demonstrating how effectively the model generalises to new, previously unseen text examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 74,
     "status": "ok",
     "timestamp": 1742402415748,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "J4PONX_YHs-U",
    "outputId": "bbd7aaad-4b29-4222-b8ad-a88129e4080c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, Y, test_size=0.20, random_state=7)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f'Text Data Accuracy: {model.score(X_test, y_test) * 100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uupd1z9THs-U"
   },
   "source": [
    "### k-Fold Cross-Validation\n",
    "Let's try k-Fold Cross Validation, to get a better estimate of how good our model will be on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1742402415751,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "JZkOjUGlHs-U",
    "outputId": "283df910-9ba4-41c4-f223-a9ee3048a33a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "kfold = KFold(n_splits=10, random_state=7, shuffle=True)\n",
    "\n",
    "results = cross_val_score(model, X_tfidf, Y, cv=kfold)\n",
    "\n",
    "print(f'k-Fold Accuracy: {results.mean()*100:.3f}% ({results.std()*100:.3f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DwRHhJLUdvTA"
   },
   "source": [
    "### Leave-One-Out Cross-Validation (LOO-CV)\n",
    "Now let's see how we immplemment Leave-One-Out Cross-Validation. The code performs Leave-One-Out Cross-Validation on the training set `X` with labels `Y`.\n",
    "\n",
    "The loop iterates over each data point, using it as the test set while the remaining *n-1* samples serve as the training set. Inside the loop, the model is trained on `X_train` and `y_train` (excluding the test sample) and then makes a prediction for `X_test` (the left-out sample). The true and predicted labels are stored in `y_true` and `y_predicted`, respectively.\n",
    "\n",
    "After all iterations, the overall classification accuracy is calculated using `accuracy_score(y_true, y_predicted)` and printed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2210,
     "status": "ok",
     "timestamp": 1742402418031,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "Iquru3CPdwK_",
    "outputId": "7fc4bd08-2a51-4263-c520-6403f26eab8a"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Define Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Store predictions and actual values\n",
    "y_true, y_predicted = [], []\n",
    "\n",
    "# Perform LOO-CV\n",
    "for train_index, test_index in loo.split(X_tfidf):\n",
    "    X_train, X_test = X_tfidf[train_index], X_tfidf[test_index]\n",
    "    y_train, y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make prediction\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Store results\n",
    "    y_true.append(y_test[0])  # Append actual class\n",
    "    y_predicted.append(y_pred[0])  # Append predicted class\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_true, y_predicted)\n",
    "print(f'LOO-CV Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SN6_Pec5dwb_"
   },
   "source": [
    "### Repeated random Test-Train Splits\n",
    "The below code performs Repeated Random Test-Train Splits using `ShuffleSplit` from `sklearn.model_selection` to evaluate a model's performance. It defines `num_splits = 10`, meaning the dataset is randomly split into training and test sets 10 times, with `test_size = 0.33`, ensuring that 33% of the data is used for testing in each iteration.\n",
    "\n",
    "The `ShuffleSplit` object generates different random splits while maintaining a fixed `random_state = 7` for reproducibility. The `cross_val_score` function trains and tests the model on these splits, recording the accuracy for each iteration.\n",
    "\n",
    "Finally, the mean accuracy and standard deviation of the scores are computed using `np.mean(scores)` and `np.std(scores)`, respectively, and printed to provide a performance estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1742402418036,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "m-MM6pOadwnn",
    "outputId": "ae0850f6-45c6-4d23-b45d-2bb093bd0c20"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Define Repeated Random Test-Train Splits\n",
    "num_splits = 10  # Number of repetitions\n",
    "test_size = 0.33  # 330% of data is used for testing\n",
    "\n",
    "shuffle_split = ShuffleSplit(n_splits=num_splits, test_size=test_size, random_state=7)\n",
    "\n",
    "# Evaluate model using repeated random test-train splits\n",
    "scores = cross_val_score(model, X_tfidf, Y, cv=shuffle_split)\n",
    "\n",
    "# Print the results\n",
    "print(f'Mean Accuracy: {np.mean(scores):.3f}, Standard Deviation: {np.std(scores):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLDQmTnnHs-U"
   },
   "source": [
    "## Resampling for Image data\n",
    "\n",
    "Exploring resampling techniques on an **image dataset**, such as the Cats vs. Dogs dataset, is slightly different from numerical or text datasets because images typically require more complex preprocessing (such as resizing, feature extraction, or encoding) before training a model.\n",
    "\n",
    "However, the core approach remains similar: we still apply resampling methods like train/test splits, k-fold cross-validation, LOOCV, and shuffle splits to assess how reliably our image classifier performs.\n",
    "\n",
    "Even though handling images may involve additional steps like converting pixel data into features the model can interpret, the fundamental goal of resampling remains unchanged—evaluating the model’s ability to generalise to new, unseen examples.\n",
    "\n",
    "When we use resampling with images, we ensure our model accurately distinguishes between categories (in this case, cats and dogs) rather than memorising specific images, resulting in a robust and trustworthy evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Amvn2UhMSDIo"
   },
   "source": [
    "### Downloading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1477,
     "status": "ok",
     "timestamp": 1742402419513,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "-uK8bqFBSDPw",
    "outputId": "0e2976f5-d036-4acd-9142-4fd664a7be47"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import zipfile\n",
    "\n",
    "# Define URL and target filenames\n",
    "url = \"https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip\"\n",
    "zip_path = \"cats_and_dogs_filtered.zip\"\n",
    "extract_dir = \"cats_dogs\"\n",
    "\n",
    "# Download the zip file\n",
    "print(\"Downloading dataset...\")\n",
    "urllib.request.urlretrieve(url, zip_path)\n",
    "print(\"Download complete.\")\n",
    "\n",
    "# Extract the zip file\n",
    "print(\"Extracting files...\")\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_dir)\n",
    "print(\"Extraction complete.\")\n",
    "\n",
    "# Delete the zip file\n",
    "print(\"Cleaning up...\")\n",
    "os.remove(zip_path)\n",
    "print(\"Cleanup complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WnGb5v24kTEj"
   },
   "source": [
    "#### Loading the data\n",
    "The code below, loads and preprocesses an image dataset (Cats vs. Dogs) for machine learning. It loops through the images stored in folders named 'cats' and 'dogs', reading each image using OpenCV (`cv2`). Each image is resized to a consistent shape of 64x64 pixels to ensure uniformity.\n",
    "\n",
    "These resized images are collected into an array `X`, while their labels (`0` for cat, `1` for dog) are stored in the array `Y`.\n",
    "\n",
    "Afterwards, the image data is converted into NumPy arrays and normalised by dividing pixel values by `255.0`, scaling them between `0` and `1` to help the model learn more effectively.\n",
    "\n",
    "We must then flatten each image from (64, 64, 3) to (64*64*3,) before training when using models like Logistic Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1742402419873,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "FCI73KuCHs-U",
    "outputId": "937c1d57-2b7c-47e4-bf14-47865a748e4a"
   },
   "outputs": [],
   "source": [
    "import os                    # File and directory handling\n",
    "import cv2                   # Image loading and processing with OpenCV\n",
    "import numpy as np           # Numerical operations on arrays\n",
    "import matplotlib.pyplot as plt  # Visualisation and plotting (useful later)\n",
    "\n",
    "# Root directory containing training images\n",
    "dataset_path = \"cats_dogs/cats_and_dogs_filtered/train/\"\n",
    "\n",
    "# Categories corresponding to subfolders in the dataset ('cats' and 'dogs')\n",
    "category_path = [\"cats\", \"dogs\"]\n",
    "IMG_SIZE = 128  # Target size for resizing images\n",
    "\n",
    "# Limit number of images per category (for faster processing)\n",
    "limit = 250\n",
    "\n",
    "# Lists to store image data (X) and labels (Y)\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# Loop through categories and load images\n",
    "for category in category_path:\n",
    "    path = os.path.join(dataset_path, category)  # Path to current category\n",
    "    count = 0\n",
    "    for file in os.listdir(path):  # Iterate over files in category folder\n",
    "        if file.endswith(('.jpg', '.jpeg', '.png')):  # Only process image files\n",
    "            img_path = os.path.join(path, file)  # Full path to image file\n",
    "            img = cv2.imread(img_path)  # Load image\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGBA)\n",
    "            img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))  # Resize image to fixed size\n",
    "            X.append(img)  # Store image data\n",
    "            Y.append(category)  # Store corresponding label ('cats' or 'dogs')\n",
    "            count += 1\n",
    "            if count >= limit:  # Stop after reaching sample limit\n",
    "                break\n",
    "\n",
    "# Convert lists into NumPy arrays for further processing\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# Flatten images from (128,128,3) into vectors (128*128*3), suitable for ML models\n",
    "X_flattened = X.reshape(len(X), -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YPODX_aHs-U"
   },
   "source": [
    "*Note*: We have selected a small sample of the images as the training data to demonstrate the principles. However, this means that the accuracy of our model will be quite poor. For better results you can use the full dataset if you have the resources. In addition, classical machine learning algorithms, are not the best model -- we would want to look at neural network models like Convolutional Neural Networks (CNNs) to achieve better performance.\n",
    "\n",
    "In any case, let's quickly preview a few images from the dataset to check everything loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 508
    },
    "executionInfo": {
     "elapsed": 1870,
     "status": "ok",
     "timestamp": 1742402421744,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "e8bx2IOTHs-U",
    "outputId": "54ae314a-c4a4-48de-95ad-dfdd641d45c7"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# Combine image arrays and labels into a list of tuples\n",
    "combined = list(zip(X, Y))\n",
    "\n",
    "# Randomly select 20 image-label pairs\n",
    "sampled = random.sample(combined, 20)\n",
    "\n",
    "# Display the randomly selected images\n",
    "fig, axes = plt.subplots(4, 5, figsize=(10, 8))  # 4 rows, 5 columns\n",
    "\n",
    "for (img, label), ax in zip(sampled, axes.flatten()):\n",
    "    # Show image\n",
    "    ax.imshow(img)\n",
    "    ax.set_title(f\"Label: {label}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQTNulzhHs-V"
   },
   "source": [
    "#### Train/Test Split\n",
    "This code applies a train/test split, as before, to the image dataset you've loaded and preprocessed. For the training data `(X_train, y_train)`, we use 80% of the datasetto train the model. Test data `(X_test, y_test)` is assigned the remaining 20%, which we reserve to evaluate the model's performance on unseen images.\n",
    "\n",
    "We'll need to flatten the images so that each image (originally 3D: height × width × channels) becomes a 1D vector. This is necessary because most classical ML models like logistic regression expect 2D input: `(samples, features)`.\n",
    "\n",
    "The parameter `random_state=seed`, ensures that the data split is reproducible. The print statement then displays how many images are allocated to the training and test sets, giving you a quick check to confirm the data has been split correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 281,
     "status": "ok",
     "timestamp": 1742402422027,
     "user": {
      "displayName": "Martyn Harris",
      "userId": "17134958627456907188"
     },
     "user_tz": 0
    },
    "id": "BMKTJ8MVHs-V",
    "outputId": "5de76317-6e64-42db-ed19-cc2a5446ee32"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "seed = 7\n",
    "\n",
    "# Train/Test split (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flattened, Y, test_size=0.2, random_state=seed)\n",
    "\n",
    "# Create and train the logistic regression model\n",
    "model = LogisticRegression(solver='liblinear', max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate and print accuracy\n",
    "print(f'Image Data Accuracy: {model.score(X_test, y_test) * 100:.3f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test with folders\n",
    "\n",
    "When working with real-world image datasets, it's common to find the data already neatly separated into *training* and *test* (or *validation*) folders. This helps make things simpler because the images have already been organised for us into separate groups. For instance, a typical folder structure might look like this:\n",
    "\n",
    "```\n",
    "cats_dogs/cats_and_dogs_filtered/\n",
    "├── train/\n",
    "│   ├── cats/\n",
    "│   └── dogs/\n",
    "├── validation/\n",
    "│   ├── cats/\n",
    "│   └── dogs/\n",
    "```\n",
    "\n",
    "Here, images of cats and dogs intended for *training* your machine learning model are stored separately from images intended for *validating* how well your model performs.\n",
    "\n",
    "In this scenario:\n",
    "\n",
    "- The *training set* contains images your model learns from. Looking at these images, our model will gradually learn to recognise distinguishing features—such as the shape of a cat’s ears or the length of a dog's nose.\n",
    "- The *validation set* (also often called the test set) contains images your model hasn't seen before. After your model has finished learning, you show it these new images to test how accurately it can classify unseen data, effectively checking if it has genuinely learned general patterns or simply memorised the training images.\n",
    "\n",
    "Splitting a dataset into training and validation sets is usually done manually. It involves carefully inspecting images and ensuring each set accurately represents the variety present in your data. For example, you wouldn't want all long-haired cats in your training set and all short-haired cats in your validation set—this could confuse the model when it tries to generalise from training to validation images. Having a good balance and sufficient data in both sets helps your model learn better and provides a clearer measure of how well it performs.\n",
    "\n",
    "Since our dataset is already organised clearly into these folders, the next step is straightforward: we simply load the images from the *training* and *validation* folders into separate arrays or lists, ready for training and evaluating our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Set image size and batch size\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Define the root path to the training dataset\n",
    "dataset_path = \"cats_dogs/cats_and_dogs_filtered/\"  # Adjust this to your dataset location\n",
    "\n",
    "# Define dataset directories\n",
    "train_dir = dataset_path + 'train'\n",
    "val_dir = dataset_path + 'validation'\n",
    "\n",
    "# Load and preprocess images directly from directories - training\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='binary'  # because we have two classes: cat/dog\n",
    ")\n",
    "\n",
    "# Load and preprocess images directly from directories - validation\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    val_dir,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='binary'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate how to use the training and validation sets with a more appropriate model, a Convolutional Neural Network (CNN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Improve performance with prefetching\n",
    "AUTOTUNE = tf.data.AUTOTUNE  # Let TensorFlow choose the optimal buffer size\n",
    "\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)  # Cache and prefetch training data\n",
    "\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)  # Cache and prefetch validation data\n",
    "\n",
    "# Define a simple CNN (Convolutional Neural Network) model\n",
    "model = models.Sequential([\n",
    "    layers.Rescaling(1./255, input_shape=(IMG_SIZE, IMG_SIZE, 3)),  # Normalise pixel values to [0, 1]\n",
    "    layers.Conv2D(32, (3, 3), activation='relu'),  # First convolutional layer\n",
    "    layers.MaxPooling2D(),                        # Downsample with max pooling\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),  # Second convolutional layer\n",
    "    layers.MaxPooling2D(),                        # Downsample again\n",
    "    layers.Flatten(),                             # Flatten feature maps into a single vector\n",
    "    layers.Dense(64, activation='relu'),          # Fully connected layer\n",
    "    layers.Dense(1, activation='sigmoid')         # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Compile the model with optimiser, loss function, and evaluation metric\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',  # Suitable for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model on the training set, validating on the validation set\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=5)\n",
    "\n",
    "# Evaluate the model on the validation data and print accuracy\n",
    "val_loss, val_acc = model.evaluate(val_ds)\n",
    "print(f'\\nValidation Accuracy: {val_acc * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27iijJDlHs-V"
   },
   "source": [
    "### What have we learnt?\n",
    "In our exploration of resampling techniques across three diverse datasets—numerical (Pima Indians Diabetes), textual (Sentiment Analysis), and image-based (Cats vs. Dogs)—we observed how methods like Train/Test Split, k-Fold Cross-Validation, Leave-One-Out Cross-Validation (LOOCV), and Shuffle Split provide reliable ways to evaluate machine learning models.  You will have noticed that regardless of the dataset used, the process and the code is pretty much the same.\n",
    "\n",
    "For each dataset, applying these resampling methods allowed us to clearly measure both the accuracy and consistency (standard deviation) of our logistic regression model. We found that while a simple Train/Test split offers quick, straightforward insights, k-Fold Cross-Validation gives a more balanced, stable, and trustworthy estimate of model performance, particularly useful when the dataset is limited or prone to variability.\n",
    "\n",
    "Whilst these approaches are suitable for numeric and language data, for image data, you would likely use a Neural Network such as a Convolutional Neural Network (CNN). Therefore, sampling involves creating separate train and test collections, with a good number of image sample sdistributed into the relevant folders and loaded separately.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
